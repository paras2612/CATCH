{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW, PreTrainedTokenizer\n",
    "from collections import namedtuple\n",
    "import json\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from model import BARTVAEClassifier, BARTDecoderClassifier, BARTVADVAEClassifier, RobertaClassifier\n",
    "from utils import ErcTextDataset, get_num_classes, get_label_VAD, convert_label_to_VAD, save_latent_params, compute_VAD_pearson_correlation, replace_for_robust_eval\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import yaml\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report, \\\n",
    "    precision_recall_fscore_support, precision_score, recall_score\n",
    "import torch.cuda.amp.grad_scaler as grad_scaler\n",
    "import torch.cuda.amp.autocast_mode as autocast_mode\n",
    "from modeling_bart import BartModel, BartDecoder\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softplus\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, RobertaForMaskedLM, AutoModel, AutoTokenizer, AutoConfig, BartTokenizer, BartConfig,RobertaForSequenceClassification, BartForSequenceClassification\n",
    "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding, BartEncoderLayer, BartDecoderLayer, _expand_mask, \\\n",
    "    _make_causal_mask, shift_tokens_right\n",
    "import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class EncodedDataset(Dataset):\n",
    "\n",
    "  def __init__(self, input_sents: List[str], \n",
    "                input_labels: List[int], \n",
    "                target_labels:List[int], \n",
    "                encoder_tokenizer: PreTrainedTokenizer,\n",
    "                decoder_tokenizer: PreTrainedTokenizer,\n",
    "                max_sequence_length: int = None, \n",
    "                max_targets: int = 8):\n",
    "      \n",
    "    self.input_sents = input_sents\n",
    "    self.input_labels = input_labels\n",
    "    self.target_labels = target_labels\n",
    "    self.encoder_tokenizer = encoder_tokenizer\n",
    "    self.decoder_tokenizer = decoder_tokenizer\n",
    "    self.max_sequence_length = max_sequence_length\n",
    "    self.max_targets = max_targets\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_sents) \n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    text = self.input_sents[index]\n",
    "    label = self.input_labels[index]\n",
    "    labels = np.zeros(2)\n",
    "    labels[label] = 1\n",
    "    target = self.target_labels[index]\n",
    "    target_labels = np.zeros(8)\n",
    "    target_labels[target] = 1\n",
    "    target_labels = torch.tensor(target_labels)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    token = self.encoder_tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
    "\n",
    "    input_ids, mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
    "\n",
    "    if self.decoder_tokenizer.pad_token is None:\n",
    "      self.decoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    token = self.decoder_tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
    "\n",
    "    dec_input_ids, dec_mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
    "\n",
    "    return input_ids, mask_ids, target_labels, labels, dec_input_ids, dec_mask_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Inference(nn.Sequential):\n",
    "    def __init__(self, num_input_channels, latent_dim, disc_variable=True):\n",
    "        super(_Inference, self).__init__()\n",
    "        if disc_variable:\n",
    "            self.add_module('fc', nn.Linear(num_input_channels, num_input_channels//2))\n",
    "            self.add_module('relu', nn.ReLU())\n",
    "            self.add_module('fc2', nn.Linear(num_input_channels//2, latent_dim))\n",
    "            self.add_module('log_softmax', nn.LogSoftmax(dim=1))\n",
    "        else:\n",
    "            self.add_module('fc', nn.Linear(num_input_channels, latent_dim))\n",
    "\n",
    "class Sample(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(Sample, self).__init__()\n",
    "        self._temperature = temperature\n",
    "\n",
    "    def forward(self, norm_mean, norm_log_sigma, disc_log_alpha, disc_label=None, mixup=False, disc_label_mixup=None,\n",
    "                mixup_lam=None):\n",
    "        \"\"\"\n",
    "        :param norm_mean: mean parameter of continuous norm variable\n",
    "        :param norm_log_sigma: log sigma parameter of continuous norm variable\n",
    "        :param disc_log_alpha: log alpha parameter of discrete multinomial variable\n",
    "        :param disc_label: the ground truth label of discrete variable (not one-hot label)\n",
    "        :param mixup: if we do mixup\n",
    "        :param disc_label_mixup: the mixup target label\n",
    "        :param mixup_lam: the mixup lambda\n",
    "        :return: sampled latent variable\n",
    "        \"\"\"\n",
    "        batch_size = norm_mean.size(0)\n",
    "        latent_sample = list([])\n",
    "        latent_sample.append(self._sample_norm(norm_mean, norm_log_sigma))\n",
    "        if disc_label is not None:\n",
    "            disc_label = torch.argmax(disc_label, dim=1)            \n",
    "            # it means we have the real label, then we can use real label instead of sampling\n",
    "            # c: N*c onehot\n",
    "            if mixup:\n",
    "                c_a = torch.zeros(disc_log_alpha.size()).cuda()\n",
    "                c_a = c_a.scatter(1, disc_label.view(-1, 1), 1)\n",
    "                c_b = torch.zeros(disc_log_alpha.size()).cuda()\n",
    "                c_b = c_b.scatter(1, disc_label_mixup.view(-1, 1), 1)\n",
    "                c = mixup_lam * c_a + (1 - mixup_lam) * c_b\n",
    "            else:\n",
    "                disc_label = disc_label.long()\n",
    "                c = torch.zeros(disc_log_alpha.size()).cuda()\n",
    "                c = c.scatter(1, disc_label, 1)\n",
    "            latent_sample.append(c)\n",
    "        else:\n",
    "            latent_sample.append(self._sample_gumbel_softmax(disc_log_alpha))\n",
    "        latent_sample = torch.cat(latent_sample, dim=1)\n",
    "        dim_size = latent_sample.size(1)\n",
    "        latent_sample = latent_sample.view(batch_size, dim_size, 1, 1)\n",
    "        return latent_sample\n",
    "\n",
    "    def _sample_gumbel_softmax(self, log_alpha):\n",
    "        \"\"\"\n",
    "        Samples from a gumbel-softmax distribution using the reparameterization\n",
    "        trick.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_alpha : torch.Tensor\n",
    "            Parameters of the gumbel-softmax distribution. Shape (N, D)\n",
    "        \"\"\"\n",
    "        EPS = 1e-12\n",
    "        unif = torch.rand(log_alpha.size()).cuda()\n",
    "        gumbel = -torch.log(-torch.log(unif + EPS) + EPS)\n",
    "        # Reparameterize to create gumbel softmax sample\n",
    "        logit = (log_alpha + gumbel) / self._temperature\n",
    "        return torch.softmax(logit, dim=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sample_norm(mu, log_sigma):\n",
    "        \"\"\"\n",
    "        :param mu: the mu for sampling with N*D\n",
    "        :param log_sigma: the log_sigma for sampling with N*D\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        std_z = torch.randn(mu.size())\n",
    "        if mu.is_cuda:\n",
    "            std_z = std_z.cuda()\n",
    "\n",
    "        return mu + torch.exp(log_sigma) * std_z\n",
    "\n",
    "\n",
    "\n",
    "class CATCH(nn.Module):\n",
    "    \"\"\"The VAD-VAE model.\"\"\"\n",
    "    def __init__(self, temperature,enc_chk,check_point, dec_chk,bart_check_point,num_targets, num_class, beta_c,beta_d, device, batch_size, decoder_type, x_sigma=1):\n",
    "        super(CATCH, self).__init__()\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.decoder_type = decoder_type\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature, requires_grad=True, device=device))\n",
    "\n",
    "        #Prepare the encoder and decoder for VAE.\n",
    "        # self.encoder = RobertaForSequenceClassification.from_pretrained(check_point).to(device)\n",
    "        if(enc_chk!= \"\"):\n",
    "            self.encoder = RobertaForSequenceClassification.from_pretrained(check_point).to(device)\n",
    "            self.encoder.load_state_dict(torch.load(enc_chk))\n",
    "            self.encoder = self.encoder.roberta\n",
    "        else:\n",
    "            self.encoder = RobertaModel.from_pretrained(check_point).to(device)\n",
    "        for param in self.encoder.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(check_point)\n",
    "        self.config = AutoConfig.from_pretrained(check_point)\n",
    "\n",
    "        self.cmi = torch.nn.Parameter(torch.rand(1, requires_grad=True, device=device))\n",
    "        self.dmi = torch.nn.Parameter(torch.rand(1, requires_grad=True, device=device))\n",
    "\n",
    "        hidden_size = self.config.hidden_size\n",
    "        if decoder_type == 'BART':\n",
    "            self.decoder = BartDecoder.from_pretrained(bart_check_point).to(device)\n",
    "            # self.decoder = BartForSequenceClassification.from_pretrained(bart_check_point).to(device)\n",
    "            # self.decoder.load_state_dict(torch.load(dec_chk))\n",
    "            # self.decoder = self.decoder.model.decoder\n",
    "        else:\n",
    "            self.decoder = nn.LSTM(hidden_size, hidden_size, 1, batch_first=True)\n",
    "\n",
    "        self.decoder_start_token_id = 2\n",
    "        self.lm_head = nn.Linear(hidden_size, self.config.vocab_size)\n",
    "        self.lm_loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.x_sigma = x_sigma\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout_rate = params1.classifier_dropout\n",
    "\n",
    "        #Prepare the disentanglement modules.\n",
    "        self.continuous_inference = nn.Sequential()\n",
    "        self.disc_latent_inference = nn.Sequential()\n",
    "        conti_mean_inf_module = _Inference(num_input_channels=hidden_size,\n",
    "                                           latent_dim=hidden_size-num_targets,\n",
    "                                           disc_variable=False)\n",
    "        conti_logsigma_inf_module = _Inference(num_input_channels=hidden_size,\n",
    "                                               latent_dim=hidden_size-num_targets,\n",
    "                                               disc_variable=False)\n",
    "        self.continuous_inference.add_module(\"mean\", conti_mean_inf_module)\n",
    "        self.continuous_inference.add_module(\"log_sigma\", conti_logsigma_inf_module)\n",
    "\n",
    "        self._disc_latent_dim = num_targets\n",
    "        dic_inf = _Inference(num_input_channels=hidden_size, latent_dim=self._disc_latent_dim,\n",
    "                             disc_variable=True)\n",
    "        self.disc_latent_inference = dic_inf\n",
    "        sample = Sample(temperature=self.temperature)\n",
    "        self.sample = sample\n",
    "\n",
    "        self.kl_beta_c = beta_c\n",
    "        self.kl_beta_d = beta_d\n",
    "\n",
    "        self.disc_log_prior_param = torch.log(\n",
    "            torch.tensor([1 / self._disc_latent_dim for i in range(self._disc_latent_dim)]).view(1, -1).float().cuda())\n",
    "\n",
    "\n",
    "        #Reconstructor\n",
    "        self.reconstructor = nn.Linear(hidden_size, hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = hidden_size)\n",
    "        #Hate Classifier\n",
    "        self.hate_classifier = nn.Sequential(\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size-num_targets,hidden_size-num_targets),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size-num_targets, num_class),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        #Target Classifier\n",
    "        self.target_classifier = nn.Sequential(\n",
    "            # nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(num_targets,num_targets),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def get_lm_loss(self, logits, labels, masks):\n",
    "        '''Get the utterance reconstruction loss.'''\n",
    "        # labels = labels.float()\n",
    "        loss = self.lm_loss_fn(logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "        masked_loss = loss * masks.view(-1)\n",
    "        return torch.mean(masked_loss)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, mask, decoder_inputs, decoder_masks, decoder_labels, vad_labels, labels):\n",
    "        \"\"\"\n",
    "        :param inputs: The input of PLM. Dim: [B, seq_len]\n",
    "        :param mask: The mask for input x. Dim: [B, seq_len]\n",
    "        \"\"\"\n",
    "        '''decoder_input_ids = shift_tokens_right(\n",
    "            x, self.config.pad_token_id, self.decoder_start_token_id\n",
    "        )'''\n",
    "        x = self.encoder(inputs, attention_mask=mask)[0]\n",
    "        x = x[:, 0, :].squeeze(1)\n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        #Get the latent variables.\n",
    "        norm_mean = self.continuous_inference.mean(x)\n",
    "        norm_log_sigma = self.continuous_inference.log_sigma(x)\n",
    "        disc_log_alpha = self.disc_latent_inference(x)\n",
    "        latent_sample = self.sample(norm_mean, norm_log_sigma, disc_log_alpha)\n",
    "        \n",
    "        latent_sample = torch.squeeze(latent_sample)\n",
    "        decoder_hidden = self.reconstructor(latent_sample)\n",
    "        \n",
    "        if self.decoder_type == 'BART':\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_inputs,\n",
    "                attention_mask=decoder_masks,\n",
    "                encoder_hidden_states=decoder_hidden.unsqueeze(1))\n",
    "            lm_logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        else:\n",
    "            input_embeddings = self.encoder.embeddings(decoder_inputs)\n",
    "            h = decoder_hidden.unsqueeze(0)\n",
    "            decoder_outputs, (_, _) = self.decoder(input_embeddings, (h, torch.zeros(h.shape).to(self.device)))\n",
    "            lm_logits = self.lm_head(decoder_outputs)\n",
    "\n",
    "        # reconstruct_loss = F.mse_loss(lm_logits, x, reduction=\"mean\") / (2 * self.batch_size * (self.x_sigma ** 2))\n",
    "        # preds = F.gumbel_softmax(lm_logits, tau=1, hard=False)\n",
    "        reconstruct_loss = self.get_lm_loss(lm_logits, decoder_labels, decoder_masks)#/ (2 * self.batch_size * (self.x_sigma ** 2))\n",
    "\n",
    "\n",
    "        # calculate latent space KL divergence\n",
    "        z_mean_sq = norm_mean * norm_mean\n",
    "        z_log_sigma_sq = 2 * norm_log_sigma\n",
    "        z_sigma_sq = torch.exp(z_log_sigma_sq)\n",
    "        continuous_kl_loss = 0.5 * torch.sum(z_mean_sq + z_sigma_sq - z_log_sigma_sq - 1) / self.batch_size\n",
    "        # notice here we duplicate the 0.5 by each part\n",
    "        # disc param : log(a1),...,log(an) type\n",
    "        # disc_kl_loss = torch.sum(torch.exp(disc_log_alpha) * (disc_log_alpha - self.disc_log_prior_param)) / self.batch_size\n",
    "        \n",
    "        prior_kl_loss_l = self.kl_beta_c * torch.abs(continuous_kl_loss - self.cmi) #+ self.kl_beta_d * torch.abs(disc_kl_loss - self.dmi)\n",
    "        elbo_loss_l = reconstruct_loss + prior_kl_loss_l\n",
    "\n",
    "        #Calculate the classification loss.\n",
    "        hate_sample = self.sample._sample_norm(norm_mean, norm_log_sigma)\n",
    "        target_sample = self.sample._sample_gumbel_softmax(disc_log_alpha)\n",
    "        \n",
    "        # hate_sample = F.dropout(hate_sample, p=self.dropout_rate, training=self.training)\n",
    "        # target_sample = self.get_vad_loss(target_sample,vad_labels)\n",
    "        hate_logits = self.hate_classifier(hate_sample)\n",
    "\n",
    "        # target_labels = torch.zeros_like(vad_labels).float().cuda()\n",
    "        # target_labels[target_logits] = 1\n",
    "        \n",
    "        # print(\"Reconstruction loss\",reconstruct_loss,\"KL Continous Loss\",self.kl_beta_c * torch.abs(continuous_kl_loss - self.cmi),\"KL Discrete loss\",self.kl_beta_d * torch.abs(disc_kl_loss - self.dmi),end=\",\")\n",
    "        return elbo_loss_l[0], hate_logits, target_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hate_model,train_data, train_labels, train_target, val_data, val_labels, val_target, encoder_tokenizer, decoder_tokenizer, params1):\n",
    "    accumulation_steps = 16\n",
    "    train = EncodedDataset(input_sents=train_data, \n",
    "                    input_labels=train_labels, \n",
    "                    target_labels=train_target,  \n",
    "                    encoder_tokenizer=encoder_tokenizer,\n",
    "                    decoder_tokenizer=decoder_tokenizer, \n",
    "                    max_sequence_length=params1.max_sequence_length)\n",
    "    \n",
    "    val =  EncodedDataset(input_sents=val_data, \n",
    "                    input_labels=val_labels, \n",
    "                    target_labels=val_target, \n",
    "                    encoder_tokenizer=encoder_tokenizer,\n",
    "                    decoder_tokenizer=decoder_tokenizer, \n",
    "                    max_sequence_length=params1.max_sequence_length)\n",
    "    \n",
    "    \n",
    "    sampler = WeightedRandomSampler(params1.h_weights, len(params1.h_weights))\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=params1.train_batch_size,drop_last=True,sampler=sampler)\n",
    "    val_dataloader = DataLoader(val, batch_size=params1.val_batch_size,drop_last=True)\n",
    "\n",
    "    vae_optimizer = torch.optim.AdamW(hate_model.parameters(),lr=params1.content_lr,weight_decay=params1.weight_decay)\n",
    "    # hate_optimizer = torch.optim.AdamW(hate_model.hate_classifier.parameters(),lr=params1.content_lr,weight_decay=params1.weight_decay)\n",
    "    # target_optimizer = torch.optim.AdamW(hate_model.target_classifier.parameters(),lr=params1.content_lr,weight_decay=params1.weight_decay)\n",
    "    hate_loss = nn.CrossEntropyLoss(params1.hate_class_weights)\n",
    "    target_loss = nn.CrossEntropyLoss(params1.target_class_weights)\n",
    "\n",
    "    s_total_steps = float(10 * len(train)) / params1.train_batch_size\n",
    "    v_scheduler = get_linear_schedule_with_warmup(vae_optimizer, int(s_total_steps * params1.warmup_ratio),math.ceil(s_total_steps))\n",
    " \n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.9,patience=2)\n",
    "\n",
    "    # hate_opt = torch.optim.Adam(hate_model.parameters(), lr=params1.content_lr, weight_decay = 1e-2)\n",
    "    save_dir = \"Models/Disentanglement/\"\n",
    "\n",
    "    best_validation_accuracy = 1e-5\n",
    "    print(\"Training started!\")\n",
    "    \n",
    "    e=0\n",
    "    for epoch in range(params1.num_epochs):\n",
    "        total_vae_loss = 0\n",
    "        total_adversary_loss = 0\n",
    "        total_cls_loss = 0\n",
    "        total_acc_train = 0\n",
    "        total_acc_target = 0\n",
    "        predictions = []\n",
    "        y_true = []\n",
    "        loss_list = []\n",
    "        predicts = []\n",
    "        ground_truth = []\n",
    "        c=0\n",
    "        cnt=0\n",
    "        hate_model.train()\n",
    "        for train_input, train_mask, train_target, train_label,train_dec_input, train_dec_mask in train_dataloader:\n",
    "            hate_model.zero_grad()\n",
    "            c+=1\n",
    "            cnt+=1\n",
    "            train_input = train_input.to(device)\n",
    "            train_mask = train_mask.to(device)\n",
    "            train_target = train_target.to(device)\n",
    "            train_label = train_label.to(device)\n",
    "            train_dec_input = train_dec_input.to(device)\n",
    "            train_dec_mask = train_dec_mask.to(device)\n",
    "            elbo_loss_l, hate_logits, target_logits = hate_model(train_input, train_mask, train_dec_input, train_dec_mask, train_input,train_target, train_label)\n",
    "            h_loss = hate_loss(hate_logits, train_label)\n",
    "            vad_loss = target_loss(target_logits,torch.argmax(train_target,dim=1))\n",
    "            # vad_loss = target_loss(target_logits, train_target)\n",
    "            # # Regularization losses\n",
    "            # l2_strength = 1e-4\n",
    "            # l2_loss = torch.tensor(0., requires_grad=True)\n",
    "            # for name, param in hate_model.named_parameters():\n",
    "            #     if 'weight' in name:\n",
    "            #         l2_loss = l2_loss + torch.norm(param, p=2)\n",
    "            # loss = h_loss + params1.alpha*elbo_loss_l + l2_strength * l2_loss + params1.target_loss_coeff*vad_loss\n",
    "            loss = params1.hate_coeff*h_loss + params1.alpha*elbo_loss_l + params1.target_loss_coeff*vad_loss\n",
    "            # loss = vad_loss\n",
    "            if((c+1)%accumulation_steps==0):\n",
    "                loss.backward()\n",
    "                vae_optimizer.step()\n",
    "                # hate_optimizer.step()\n",
    "                # target_optimizer.step()\n",
    "                v_scheduler.step()\n",
    "                # h_scheduler.step()\n",
    "                # t_scheduler.step()\n",
    "            \n",
    "            if(c%100==0):\n",
    "                print(\"Temperature\",hate_model.temperature.item(),\"batch\",c,\"epoch\",epoch,\"1s\",len(np.where(torch.argmax(hate_logits, dim=1).cpu().numpy()==1)[0]),\"loss\",loss.item(),\"h_loss\",h_loss.item(),\"elbo_loss\",params1.alpha*elbo_loss_l.item(),\"vad_loss\",params1.target_loss_coeff*vad_loss.item())\n",
    "            ground_truth += train_label.cpu().numpy().tolist()\n",
    "            predicts += torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()\n",
    "            loss_list.append(loss.item())\n",
    "            acc = round(accuracy_score(torch.argmax(train_label, dim=1).cpu().numpy().tolist(), torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "            target_acc = round(accuracy_score(torch.argmax(train_target, dim=1).cpu().numpy().tolist(), torch.argmax(target_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "            total_vae_loss += elbo_loss_l.item()\n",
    "            total_adversary_loss += vad_loss.item()\n",
    "            total_cls_loss += h_loss.item()\n",
    "            total_acc_train += acc\n",
    "            total_acc_target += target_acc\n",
    "            # print(\"PREDS: \",torch.argmax(target_logits, dim=1).cpu().numpy().tolist())\n",
    "            # print(\"TARGETS: \",torch.argmax(train_target, dim=1).cpu().numpy().tolist())\n",
    "            if(c%100==0):\n",
    "                print(\"Train Accuracy\",acc,\" Train Target Accuracy\",target_acc)\n",
    "            # if(c==500):\n",
    "            #     break\n",
    "        y_true = []\n",
    "        predictions = []\n",
    "        hate_model.eval()\n",
    "        total_loss_val = 0\n",
    "        total_acc_val = 0\n",
    "        total_acc_target_val = 0\n",
    "        e_cnt=0\n",
    "        print(\"VALIDATION\")\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_mask, val_target, val_label, val_dec_input, val_dec_mask in val_dataloader:\n",
    "                e_cnt+=1\n",
    "                val_input = val_input.to(device)\n",
    "                val_mask = val_mask.to(device)\n",
    "                val_target = val_target.to(device)\n",
    "                val_label = val_label.to(device)\n",
    "                val_dec_input = val_dec_input.to(device)\n",
    "                val_dec_mask = val_dec_mask.to(device)\n",
    "                elbo_loss_l, hate_logits, target_logits = hate_model(val_input, val_mask, val_dec_input, val_dec_mask, val_input,val_target, val_label)\n",
    "                total_acc_val += round(accuracy_score(torch.argmax(val_label, dim=1).cpu().numpy().tolist(), torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "                total_acc_target_val += round(accuracy_score(torch.argmax(val_target, dim=1).cpu().numpy().tolist(), torch.argmax(target_logits,dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "                y_true += torch.argmax(val_label, dim=1).cpu().numpy().tolist()\n",
    "                predictions += torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()\n",
    "            print(classification_report(y_true, predictions))\n",
    "\n",
    "        metrics = {\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train ELBO Loss\" : total_vae_loss/cnt,\n",
    "            \"Train Target Loss\" : total_adversary_loss/cnt,\n",
    "            \"Train Hate Loss\" : total_cls_loss/cnt,\n",
    "            \"Train Hate Accuracy\" : total_acc_train/cnt,\n",
    "            \"Train Target Accuracy\" : total_acc_target/cnt,\n",
    "            \"Validation Loss\" : total_loss_val/e_cnt,\n",
    "            \"Validation Hate Accuracy\" : total_acc_val/e_cnt,\n",
    "            \"Validation Target Accuracy\" : total_acc_target_val/e_cnt}\n",
    "\n",
    "        print(metrics)\n",
    "        wandb.log(metrics)\n",
    "        \n",
    "        val_metrics = {\"F1-Score\": classification_report(y_true, predictions,output_dict=True)['macro avg']['f1-score']}\n",
    "        wandb.log({**metrics, **val_metrics})\n",
    "\n",
    "        if(best_validation_accuracy <= round(classification_report(y_true, predictions,output_dict=True)['macro avg']['f1-score'],3)):\n",
    "            best_validation_accuracy = round(classification_report(y_true, predictions,output_dict=True)['macro avg']['f1-score'],3)\n",
    "            best_report = classification_report(y_true, predictions)\n",
    "            e=0\n",
    "            print(\"E from if = \",e)\n",
    "            # fname = \"best-model_\" + params1.dataset_name+\"_\"+str(epoch+1)+\"_VAE_with_rob_base_NoFT_balancedData.pt\"\n",
    "            fname = \"best-model_\" + params1.dataset_name+\"_\"+str(epoch+1)+params1.fname\n",
    "            torch.save(hate_model.state_dict(), os.path.join(save_dir, fname))\n",
    "            print(\"Saved at \",os.path.join(save_dir, fname))\n",
    "        else:\n",
    "            print(\"E = \",e)\n",
    "            e+=1\n",
    "        if(e==3):\n",
    "            print(best_report)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/Preprocessed Datasets/GAB/gab_HX_Numeric_train.csv', 'C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/Preprocessed Datasets/Reddit/reddit_TRY_Numeric_train.csv', 'C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/Preprocessed Datasets/Twitter/twitter_HX_Numeric_train.csv', 'C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/Preprocessed Datasets/YouTube/youtube_IC_Numeric_balanced.csv']\n",
      "4\n",
      "cuda:0\n",
      "[  0.34374547   0.41431481   1.32786274   2.0465651    2.37096237\n",
      " 101.71428571 254.28571429   0.        ]\n",
      "tensor([  0.3437,   0.4143,   1.3279,   2.0466,   2.3710, 101.7143, 254.2857,\n",
      "          0.0000], device='cuda:0')\n",
      "TEST FILE:  C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/Preprocessed Datasets/Reddit/reddit_TRY_Numeric_test.csv\n",
      "tensor([0.1629, 0.1629, 0.1629,  ..., 0.1629, 0.1629, 0.1629])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 15:07:45.933 ERROR jupyter - notebook_metadata: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mps2612\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110398f5feee49cfb43bceeb983dbda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\psheth5\\Downloads\\Cross-Target Hate Speech Datasets\\BuildingFramework\\wandb\\run-20230714_150747-32cllwje</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ps2612/Disentangling%20Hate%20Speech%20and%20Target/runs/32cllwje' target=\"_blank\">experiment_ROB_BART_gab_HX_0.2_0.005</a></strong> to <a href='https://wandb.ai/ps2612/Disentangling%20Hate%20Speech%20and%20Target' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ps2612/Disentangling%20Hate%20Speech%20and%20Target' target=\"_blank\">https://wandb.ai/ps2612/Disentangling%20Hate%20Speech%20and%20Target</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ps2612/Disentangling%20Hate%20Speech%20and%20Target/runs/32cllwje' target=\"_blank\">https://wandb.ai/ps2612/Disentangling%20Hate%20Speech%20and%20Target/runs/32cllwje</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/FineTuned/Roberta/gab_HX/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartDecoder: ['decoder.layers.4.fc2.weight', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.5.final_layer_norm.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.0.encoder_attn.v_proj.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.2.fc1.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.0.fc1.weight', 'decoder.layers.1.encoder_attn.k_proj.bias', 'encoder.layers.1.fc1.bias', 'decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.bias', 'decoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.q_proj.bias', 'decoder.embed_positions.weight', 'encoder.layers.2.fc2.weight', 'decoder.layers.4.final_layer_norm.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.4.final_layer_norm.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.5.encoder_attn.k_proj.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layernorm_embedding.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.1.encoder_attn.out_proj.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.1.fc1.bias', 'decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.layers.0.encoder_attn.k_proj.bias', 'decoder.layers.1.fc2.weight', 'decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.layernorm_embedding.bias', 'decoder.layers.5.fc2.bias', 'decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.1.fc2.bias', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.2.fc2.bias', 'decoder.layers.0.encoder_attn.out_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.3.encoder_attn.k_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.4.encoder_attn.q_proj.bias', 'encoder.layers.5.fc1.bias', 'decoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc2.bias', 'decoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.q_proj.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.3.fc1.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.weight', 'decoder.layers.3.encoder_attn_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.0.fc2.bias', 'decoder.layers.3.fc2.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.weight', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.fc1.bias', 'encoder.layers.4.fc1.weight', 'decoder.layers.3.encoder_attn.q_proj.bias', 'decoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.5.fc1.weight', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.v_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.embed_tokens.weight', 'decoder.layers.4.encoder_attn_layer_norm.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.3.fc2.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.1.fc1.weight', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.layers.2.final_layer_norm.weight', 'shared.weight', 'decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.layers.2.encoder_attn.out_proj.bias', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.4.fc1.weight', 'decoder.layers.4.encoder_attn.out_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.v_proj.weight', 'encoder.embed_tokens.weight', 'decoder.layers.3.fc1.weight', 'encoder.layers.5.fc2.bias', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.2.encoder_attn.v_proj.bias', 'encoder.layers.1.final_layer_norm.bias', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.2.fc1.weight', 'encoder.layers.1.fc1.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.0.encoder_attn.q_proj.bias', 'decoder.layers.1.fc2.bias', 'decoder.layers.0.encoder_attn.q_proj.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.4.fc2.weight', 'decoder.layers.2.encoder_attn.q_proj.bias', 'decoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.5.final_layer_norm.bias', 'decoder.layers.4.final_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.layers.4.encoder_attn_layer_norm.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.final_layer_norm.bias', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.4.fc2.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.5.encoder_attn.k_proj.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.layernorm_embedding.weight', 'encoder.embed_positions.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.0.fc2.bias', 'decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.5.fc1.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.2.fc2.weight', 'decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.layers.5.fc2.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.4.encoder_attn.k_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.2.encoder_attn_layer_norm.bias', 'encoder.layernorm_embedding.weight', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.4.fc1.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.5.fc1.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.5.encoder_attn_layer_norm.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.0.fc1.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.4.fc1.bias', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.3.encoder_attn.out_proj.bias', 'encoder.layers.0.fc1.bias', 'encoder.layers.4.final_layer_norm.bias', 'decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.4.encoder_attn.out_proj.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.2.fc2.bias', 'decoder.layers.5.encoder_attn.out_proj.weight', 'encoder.layers.2.final_layer_norm.weight', 'decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.layers.0.final_layer_norm.weight', 'encoder.layers.5.fc2.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing BartDecoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartDecoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartDecoder were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['layers.3.fc1.bias', 'layers.1.encoder_attn.out_proj.bias', 'layers.3.encoder_attn.out_proj.weight', 'layers.0.self_attn.k_proj.bias', 'layers.4.self_attn.q_proj.weight', 'layers.3.self_attn_layer_norm.bias', 'layers.3.fc2.bias', 'layers.5.fc1.bias', 'layers.0.fc2.weight', 'layers.1.self_attn.k_proj.weight', 'layers.4.self_attn.out_proj.weight', 'layers.0.self_attn.q_proj.bias', 'layers.2.self_attn.v_proj.bias', 'layers.5.self_attn_layer_norm.bias', 'layers.3.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.k_proj.weight', 'layers.5.encoder_attn.q_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.fc2.bias', 'layers.5.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.k_proj.bias', 'embed_tokens.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.self_attn.k_proj.bias', 'layers.2.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.3.self_attn.k_proj.weight', 'layers.2.final_layer_norm.weight', 'layers.5.self_attn_layer_norm.weight', 'layers.1.self_attn_layer_norm.weight', 'layers.4.self_attn_layer_norm.bias', 'layers.4.encoder_attn_layer_norm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.5.self_attn.v_proj.bias', 'layers.0.encoder_attn_layer_norm.bias', 'layers.2.encoder_attn.q_proj.bias', 'layers.3.encoder_attn_layer_norm.bias', 'layers.4.fc2.bias', 'layers.5.encoder_attn.k_proj.bias', 'layers.3.encoder_attn.out_proj.bias', 'layers.0.self_attn.v_proj.bias', 'layers.0.encoder_attn.q_proj.weight', 'layers.1.final_layer_norm.weight', 'layers.3.self_attn.q_proj.bias', 'layers.2.self_attn_layer_norm.bias', 'layers.5.encoder_attn_layer_norm.bias', 'layers.2.self_attn.k_proj.bias', 'layers.4.encoder_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.weight', 'layers.0.encoder_attn.k_proj.bias', 'layers.2.encoder_attn.v_proj.weight', 'layers.3.encoder_attn.q_proj.weight', 'layers.3.encoder_attn.q_proj.bias', 'layers.5.self_attn.q_proj.bias', 'layers.1.encoder_attn_layer_norm.bias', 'layers.1.encoder_attn.k_proj.bias', 'layers.1.fc1.bias', 'layers.2.encoder_attn.k_proj.weight', 'layers.3.fc2.weight', 'layers.0.self_attn.out_proj.weight', 'layers.4.encoder_attn.v_proj.weight', 'layers.4.final_layer_norm.weight', 'layers.3.encoder_attn.k_proj.bias', 'layernorm_embedding.weight', 'layers.3.self_attn.v_proj.bias', 'layers.4.final_layer_norm.bias', 'layers.4.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.q_proj.bias', 'layers.2.self_attn.v_proj.weight', 'layers.0.fc1.bias', 'layers.1.self_attn.v_proj.weight', 'layers.2.self_attn.out_proj.weight', 'layers.0.encoder_attn.v_proj.weight', 'layers.0.final_layer_norm.bias', 'layers.2.self_attn_layer_norm.weight', 'layers.5.fc2.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.final_layer_norm.bias', 'layers.3.encoder_attn_layer_norm.weight', 'layers.2.fc2.bias', 'layers.0.encoder_attn.out_proj.bias', 'layers.3.self_attn.k_proj.bias', 'layers.4.self_attn.q_proj.bias', 'layers.0.encoder_attn.k_proj.weight', 'layers.1.self_attn.out_proj.bias', 'layers.3.self_attn.out_proj.bias', 'layers.4.encoder_attn.out_proj.weight', 'layers.5.encoder_attn.out_proj.bias', 'layers.2.encoder_attn.out_proj.bias', 'layers.5.self_attn.q_proj.weight', 'layers.1.encoder_attn.k_proj.weight', 'layers.0.encoder_attn.v_proj.bias', 'layers.0.final_layer_norm.weight', 'layers.1.self_attn.out_proj.weight', 'layers.1.encoder_attn_layer_norm.weight', 'layers.4.self_attn.k_proj.bias', 'layers.3.self_attn_layer_norm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.1.encoder_attn.v_proj.weight', 'layers.4.encoder_attn.out_proj.bias', 'layers.1.encoder_attn.q_proj.weight', 'layers.2.encoder_attn.v_proj.bias', 'layers.4.self_attn_layer_norm.weight', 'layers.0.encoder_attn_layer_norm.weight', 'layers.0.fc1.weight', 'layers.5.self_attn.v_proj.weight', 'layers.0.self_attn_layer_norm.weight', 'layers.1.encoder_attn.q_proj.bias', 'layers.2.encoder_attn.out_proj.weight', 'layers.5.self_attn.out_proj.weight', 'layers.1.encoder_attn.v_proj.bias', 'layers.3.encoder_attn.v_proj.bias', 'layers.3.self_attn.out_proj.weight', 'layers.5.self_attn.k_proj.weight', 'layers.3.fc1.weight', 'layers.4.encoder_attn_layer_norm.bias', 'layers.0.self_attn.out_proj.bias', 'layers.2.encoder_attn_layer_norm.bias', 'layers.2.fc2.weight', 'layers.5.final_layer_norm.weight', 'layers.1.encoder_attn.out_proj.weight', 'layers.1.fc2.weight', 'layers.4.self_attn.v_proj.bias', 'layers.1.self_attn.v_proj.bias', 'layers.4.encoder_attn.q_proj.bias', 'layers.5.encoder_attn_layer_norm.weight', 'layers.0.self_attn.q_proj.weight', 'layers.2.encoder_attn.k_proj.bias', 'embed_positions.weight', 'layers.1.self_attn.k_proj.bias', 'layers.2.encoder_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.2.self_attn.q_proj.bias', 'layers.0.self_attn_layer_norm.bias', 'layers.5.final_layer_norm.bias', 'layers.5.fc1.weight', 'layers.2.encoder_attn_layer_norm.weight', 'layers.5.encoder_attn.out_proj.weight', 'layers.4.self_attn.k_proj.weight', 'layers.3.encoder_attn.v_proj.weight', 'layers.0.fc2.bias', 'layers.1.self_attn_layer_norm.bias', 'layers.2.self_attn.out_proj.bias', 'layers.2.fc1.bias', 'layers.5.encoder_attn.v_proj.bias', 'layers.2.fc1.weight', 'layers.1.self_attn.q_proj.bias', 'layers.4.fc2.weight', 'layers.5.encoder_attn.q_proj.bias', 'layers.5.fc2.bias', 'layers.3.final_layer_norm.weight', 'layers.1.fc1.weight', 'layers.1.final_layer_norm.bias', 'layernorm_embedding.bias', 'layers.0.encoder_attn.out_proj.weight', 'layers.2.final_layer_norm.bias', 'layers.4.fc1.bias', 'layers.4.self_attn.out_proj.bias', 'layers.5.self_attn.out_proj.bias', 'layers.4.fc1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gab_HX\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/Preprocessed Datasets/GAB/gab_HX_Numeric_train.csv\n",
      "(7120, 3)\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "files = ['./Preprocessed Datasets/GAB/gab_HX_Numeric_train.csv',\n",
    "           './Preprocessed Datasets/Reddit/reddit_TRY_Numeric_train.csv',\n",
    "           './Preprocessed Datasets/Twitter/twitter_HX_Numeric_train.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_train.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_Numeric_train.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_Numeric_train.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_train.csv',\n",
    "            './Preprocessed Datasets/YouTube/youtube_IC_Numeric_balanced.csv']\n",
    "\n",
    "test_files = ['./Preprocessed Datasets/GAB/gab_HX_Numeric_test.csv',\n",
    "           './Preprocessed Datasets/Reddit/reddit_TRY_Numeric_test.csv',\n",
    "           './Preprocessed Datasets/Twitter/twitter_HX_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_Numeric_test.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_Numeric_test.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_test.csv',\n",
    "            './Preprocessed Datasets/Youtube/youtube_IC_Numeric_test.csv']\n",
    "\n",
    "enc_files = ['./FineTuned/Roberta/gab_HX/pytorch_model.bin',\n",
    "            './FineTuned/Roberta/reddit_TRY/pytorch_model.bin',\n",
    "            './FineTuned/Roberta/twitter_HX/pytorch_model.bin',\n",
    "            # './FineTuned/Roberta/twitter_TRY/pytorch_model.bin',\n",
    "            # './FineTuned/Roberta/twitter/pytorch_model.bin',\n",
    "            # './FineTuned/Roberta/youtube/pytorch_model.bin',\n",
    "            # './FineTuned/Roberta/youtube_TRY/pytorch_model.bin',\n",
    "            './FineTuned/Roberta/youtube_IC/pytorch_model.bin']\n",
    "\n",
    "# enc_files = ['./FineTuned/Roberta/gab/pytorch_model.bin',\n",
    "#              './FineTuned/RobertaHate/reddit_TRY/pytorch_model.bin',\n",
    "#              './FineTuned/RobertaHate/twitter/pytorch_model.bin',\n",
    "#              './FineTuned/RobertaHate/youtube/pytorch_model.bin']\n",
    "\n",
    "dec_files = ['./FineTuned/BART/pytorch_model.bin',\n",
    "             './FineTuned/BART_reddit_TRY/checkpoint-7000/pytorch_model.bin',\n",
    "             './FineTuned/BART_twitter/pytorch_model.bin',\n",
    "             './FineTuned/BART_youtube/pytorch_model.bin']\n",
    "\n",
    "\n",
    "dataset_names = [\"gab_HX\",\"reddit_TRY\",\"twitter_HX\",\"youtube_IC\"]#\"twitter_TRY\",\"twitter\",\"youtube\",\"youtube_TRY\",\n",
    "hidden_size = 768\n",
    "classifier_dropout = 0.2\n",
    "learning_rate = 5e-3\n",
    "print(files)\n",
    "print(len(files))\n",
    "print(device)\n",
    "filenames = set()\n",
    "latent_variables = ['hate','target']#['0','1','2','3','4','5','6','7']\n",
    "\n",
    "for f in range(0,len(files)):\n",
    "        torch.cuda.empty_cache()\n",
    "        train_frame = pd.read_csv(files[f])\n",
    "        tar = np.zeros(8)\n",
    "        a1 = np.unique(train_frame['target'])\n",
    "        class_weights = compute_class_weight('balanced', classes=a1, y=train_frame['target'])\n",
    "        for i in range(len(a1)):\n",
    "            try:\n",
    "                tar[a1[i]] = class_weights[a1[i]]\n",
    "            except:\n",
    "                tar[a1[i]] = 0\n",
    "        print(tar)\n",
    "        class_weights2 = torch.FloatTensor(tar).to(device)\n",
    "        print(class_weights2)\n",
    "        if(dataset_names[f] not in filenames):\n",
    "            filenames.add(dataset_names[f])\n",
    "            # train_frame = pd.read_csv(files[f])\n",
    "            if(f+1<len(files)):\n",
    "                print(\"TEST FILE: \",test_files[f+1])\n",
    "                test_frame = pd.read_csv(test_files[f+1])\n",
    "            else:\n",
    "                 test_frame = pd.read_csv(test_files[0])\n",
    "            class_weights1 = compute_class_weight('balanced', classes=np.unique(train_frame['label']), y=train_frame['label'])\n",
    "            class_weights1 = torch.FloatTensor(class_weights1)\n",
    "            x = train_frame['label'].value_counts().values\n",
    "            class_weights3 = torch.FloatTensor([x[0]/sum(x),x[1]/sum(x)])\n",
    "            # class_weights3 = torch.FloatTensor([0.6,0.4])\n",
    "            h_weights = class_weights3[train_frame['label']]\n",
    "            print(h_weights)\n",
    "            class_weights1 = class_weights1.to(device)\n",
    "            wandb.init(\n",
    "                # Set the project where this run will be logged\n",
    "                project=\"Disentangling Hate Speech and Target\", \n",
    "                # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "                name=f\"experiment_ROB_BART_{dataset_names[f]}_{classifier_dropout}_{learning_rate}\",\n",
    "                # Track hyperparameters and run metadata\n",
    "                config={\n",
    "                        \"max_sequence_length\": 512, \n",
    "                        \"train_batch_size\" : 8, \n",
    "                        \"val_batch_size\" : 8,\n",
    "                        \"hidden_dim\" : 512, \n",
    "                        \"hate_dim\" : 384,\n",
    "                        \"num_epochs\" : 100, \n",
    "                        \"device\" : device,\n",
    "                        \"dataset_name\" : dataset_names[f],\n",
    "                        \"h_weights\" : h_weights,\n",
    "                        \"hate_class_weights\" : class_weights1,\n",
    "                        \"target_class_weights\" : class_weights2,\n",
    "                        \"hidden_size\" : 768,\n",
    "                        \"num_labels\": 2,\n",
    "                        \"num_targets\": 8,\n",
    "                        \"classifier_dropout\" : classifier_dropout,\n",
    "                        \"content_lr\": 1e-4,\n",
    "                        \"decoder_type\" : \"BART\",\n",
    "                        \"kl_weight\" : 0.05,\n",
    "                        \"mi_loss_weight\" : 0.001,\n",
    "                        \"mi_loss\" : False,\n",
    "                        \"alpha\" : 1,\n",
    "                        \"beta_c\" : 0.05,\n",
    "                        \"beta_d\" : 0.05,\n",
    "                        \"warmup_ratio\" : 0.2,\n",
    "                        \"weight_decay\" : 0.001,\n",
    "                        \"target_loss_coeff\" : 1,\n",
    "                        \"hate_coeff\" : 1,\n",
    "                        \"fname\" : \"_VAE_with_rob_base_NoFT_balancedData.pt\",\n",
    "                        })\n",
    "            params1 = { \n",
    "                        \"max_sequence_length\": 512, \n",
    "                        \"train_batch_size\" : 8, \n",
    "                        \"val_batch_size\" : 8,\n",
    "                        \"hidden_dim\" : 512, \n",
    "                        \"hate_dim\" : 384,\n",
    "                        \"num_epochs\" : 100, \n",
    "                        \"device\" : device,\n",
    "                        \"dataset_name\" : dataset_names[f],\n",
    "                        \"h_weights\" : h_weights,\n",
    "                        \"hate_class_weights\" : class_weights1,\n",
    "                        \"target_class_weights\" : class_weights2,\n",
    "                        \"hidden_size\" : 768,\n",
    "                        \"num_labels\": 2,\n",
    "                        \"num_targets\": 8,\n",
    "                        \"classifier_dropout\" : classifier_dropout,\n",
    "                        \"content_lr\": 1e-4,\n",
    "                        \"decoder_type\" : \"BART\",\n",
    "                        \"kl_weight\" : 0.05,\n",
    "                        \"mi_loss_weight\" : 0.001,\n",
    "                        \"mi_loss\" : False,\n",
    "                        \"alpha\" : 1,\n",
    "                        \"beta_c\" : 0.05,\n",
    "                        \"beta_d\" : 0.05,\n",
    "                        \"warmup_ratio\" : 0.2,\n",
    "                        \"weight_decay\" : 0.001,\n",
    "                        \"target_loss_coeff\" : 1,\n",
    "                        \"hate_coeff\" : 1,\n",
    "                        \"fname\" : \"_VAE_with_rob_base_NoFT_balancedData.pt\",\n",
    "                    }\n",
    "            \n",
    "            params1 = SimpleNamespace(**params1)\n",
    "            # enc_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "            enc_model_name = \"roberta-base\"\n",
    "            dec_model_name = \"facebook/bart-base\"\n",
    "            # enc_chk = \"./FineTuned/Roberta/pytorch_model.bin\"\n",
    "            print(enc_files[f])\n",
    "            enc_chk = enc_files[f]#\"./FineTuned/RobertaHate/pytorch_model.bin\"\n",
    "            dec_chk = dec_files[0]#\"./FineTuned/BART/pytorch_model.bin\"\n",
    "            hate_model = CATCH(1.0,enc_chk,enc_model_name,dec_chk,dec_model_name, params1.num_targets,params1.num_labels,params1.beta_c, params1.beta_d,device,params1.train_batch_size,params1.decoder_type).to(device)\n",
    "            encoder_tokenizer = RobertaTokenizer.from_pretrained(enc_model_name)\n",
    "            decoder_tokenizer = BartTokenizer.from_pretrained(dec_model_name)\n",
    "            print(params1.dataset_name)\n",
    "            print(files[f])\n",
    "            print(train_frame.shape)\n",
    "            wandb.watch(hate_model)\n",
    "            # train(hate_model=hate_model,\n",
    "            # train_data=train_frame['text'].values.tolist(), \n",
    "            # train_labels=train_frame['label'].values.tolist(), \n",
    "            # train_target=train_frame['target'].values.tolist(), \n",
    "            # val_data=test_frame['text'].values.tolist(), \n",
    "            # val_labels=test_frame['final_label'].values.tolist(), \n",
    "            # val_target = test_frame['target'].values.tolist(),\n",
    "            # encoder_tokenizer=encoder_tokenizer,\n",
    "            # decoder_tokenizer=decoder_tokenizer,\n",
    "            # params1=params1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# import pandas as pd\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from types import SimpleNamespace\n",
    "\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# files = ['./Preprocessed Datasets/GAB/gab_HX_Numeric_train.csv',\n",
    "#            './Preprocessed Datasets/Reddit/reddit_TRY_Numeric_train.csv',\n",
    "#            './Preprocessed Datasets/Twitter/twitter_HX_Numeric_train.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_train.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_Numeric_train.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_Numeric_train.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_train.csv',\n",
    "#             './Preprocessed Datasets/YouTube/youtube_IC_Numeric_balanced.csv']\n",
    "\n",
    "# test_files = ['./Preprocessed Datasets/GAB/gab_HX_Numeric_test.csv',\n",
    "#            './Preprocessed Datasets/Reddit/reddit_TRY_Numeric_test.csv',\n",
    "#            './Preprocessed Datasets/Twitter/twitter_HX_Numeric_test.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_test.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_Numeric_test.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_Numeric_test.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_test.csv',\n",
    "#             './Preprocessed Datasets/Youtube/youtube_IC_Numeric_test.csv']\n",
    "\n",
    "# enc_files = ['./FineTuned/Roberta/gab_HX/pytorch_model.bin',\n",
    "#             './FineTuned/Roberta/reddit_TRY/pytorch_model.bin',\n",
    "#             './FineTuned/Roberta/twitter_HX/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/twitter_TRY/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/twitter/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/youtube/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/youtube_TRY/pytorch_model.bin',\n",
    "#             './FineTuned/Roberta/youtube_IC/pytorch_model.bin']\n",
    "\n",
    "# # enc_files = ['./FineTuned/Roberta/gab/pytorch_model.bin',\n",
    "# #              './FineTuned/RobertaHate/reddit_TRY/pytorch_model.bin',\n",
    "# #              './FineTuned/RobertaHate/twitter/pytorch_model.bin',\n",
    "# #              './FineTuned/RobertaHate/youtube/pytorch_model.bin']\n",
    "\n",
    "# dec_files = ['./FineTuned/BART/pytorch_model.bin',\n",
    "#              './FineTuned/BART_reddit_TRY/checkpoint-7000/pytorch_model.bin',\n",
    "#              './FineTuned/BART_twitter/pytorch_model.bin',\n",
    "#              './FineTuned/BART_youtube/pytorch_model.bin']\n",
    "\n",
    "\n",
    "# dataset_names = [\"gab_HX\",\"reddit_TRY\",\"twitter_HX\",\"youtube_IC\"]#\"twitter_TRY\",\"twitter\",\"youtube\",\"youtube_TRY\",\n",
    "# hidden_size = 768\n",
    "# classifier_dropout = 0.2\n",
    "# learning_rate = 5e-3\n",
    "# print(files)\n",
    "# print(len(files))\n",
    "# print(device)\n",
    "# filenames = set()\n",
    "# latent_variables = ['hate','target']#['0','1','2','3','4','5','6','7']\n",
    "\n",
    "# for f in range(0,len(files)):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         train_frame = pd.read_csv(files[f])\n",
    "#         tar = np.zeros(8)\n",
    "#         a1 = np.unique(train_frame['target'])\n",
    "#         class_weights = compute_class_weight('balanced', classes=a1, y=train_frame['target'])\n",
    "#         for i in range(len(a1)):\n",
    "#             try:\n",
    "#                 tar[a1[i]] = class_weights[a1[i]]\n",
    "#             except:\n",
    "#                 tar[a1[i]] = 0\n",
    "#         print(tar)\n",
    "#         class_weights2 = torch.FloatTensor(tar).to(device)\n",
    "#         print(class_weights2)\n",
    "#         if(dataset_names[f] not in filenames):\n",
    "#             filenames.add(dataset_names[f])\n",
    "#             # train_frame = pd.read_csv(files[f])\n",
    "#             if(f+1<len(files)):\n",
    "#                 print(\"TEST FILE: \",test_files[f+1])\n",
    "#                 test_frame = pd.read_csv(test_files[f+1])\n",
    "#             else:\n",
    "#                  test_frame = pd.read_csv(test_files[0])\n",
    "#             class_weights1 = compute_class_weight('balanced', classes=np.unique(train_frame['label']), y=train_frame['label'])\n",
    "#             class_weights1 = torch.FloatTensor(class_weights1)\n",
    "#             x = train_frame['label'].value_counts().values\n",
    "#             class_weights3 = torch.FloatTensor([x[0]/sum(x),x[1]/sum(x)])\n",
    "#             # class_weights3 = torch.FloatTensor([0.6,0.4])\n",
    "#             h_weights = class_weights3[train_frame['label']]\n",
    "#             print(h_weights)\n",
    "#             class_weights1 = class_weights1.to(device)\n",
    "#             wandb.init(\n",
    "#                 # Set the project where this run will be logged\n",
    "#                 project=\"Disentangling Hate Speech and Target\", \n",
    "#                 # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "#                 name=f\"experiment_ROB_BART_{dataset_names[f]}_{classifier_dropout}_{learning_rate}\",\n",
    "#                 # Track hyperparameters and run metadata\n",
    "#                 config={\n",
    "#                         \"max_sequence_length\": 512, \n",
    "#                         \"train_batch_size\" : 8, \n",
    "#                         \"val_batch_size\" : 8,\n",
    "#                         \"hidden_dim\" : 512, \n",
    "#                         \"hate_dim\" : 384,\n",
    "#                         \"num_epochs\" : 100, \n",
    "#                         \"device\" : device,\n",
    "#                         \"dataset_name\" : dataset_names[f],\n",
    "#                         \"h_weights\" : h_weights,\n",
    "#                         \"hate_class_weights\" : class_weights1,\n",
    "#                         \"target_class_weights\" : class_weights2,\n",
    "#                         \"hidden_size\" : 768,\n",
    "#                         \"num_labels\": 2,\n",
    "#                         \"num_targets\": 8,\n",
    "#                         \"classifier_dropout\" : classifier_dropout,\n",
    "#                         \"content_lr\": 1e-4,\n",
    "#                         \"decoder_type\" : \"BART\",\n",
    "#                         \"kl_weight\" : 0.05,\n",
    "#                         \"mi_loss_weight\" : 0.001,\n",
    "#                         \"mi_loss\" : False,\n",
    "#                         \"alpha\" : 1,\n",
    "#                         \"beta_c\" : 0.05,\n",
    "#                         \"beta_d\" : 0.05,\n",
    "#                         \"warmup_ratio\" : 0.2,\n",
    "#                         \"weight_decay\" : 0.001,\n",
    "#                         \"target_loss_coeff\" : 1,\n",
    "#                         \"hate_coeff\" : 0,\n",
    "#                         \"fname\" : \"_VAE_with_rob_base_NoHL_balancedData.pt\",\n",
    "#                         })\n",
    "#             params1 = { \n",
    "#                         \"max_sequence_length\": 512, \n",
    "#                         \"train_batch_size\" : 8, \n",
    "#                         \"val_batch_size\" : 8,\n",
    "#                         \"hidden_dim\" : 512, \n",
    "#                         \"hate_dim\" : 384,\n",
    "#                         \"num_epochs\" : 100, \n",
    "#                         \"device\" : device,\n",
    "#                         \"dataset_name\" : dataset_names[f],\n",
    "#                         \"h_weights\" : h_weights,\n",
    "#                         \"hate_class_weights\" : class_weights1,\n",
    "#                         \"target_class_weights\" : class_weights2,\n",
    "#                         \"hidden_size\" : 768,\n",
    "#                         \"num_labels\": 2,\n",
    "#                         \"num_targets\": 8,\n",
    "#                         \"classifier_dropout\" : classifier_dropout,\n",
    "#                         \"content_lr\": 1e-4,\n",
    "#                         \"decoder_type\" : \"BART\",\n",
    "#                         \"kl_weight\" : 0.05,\n",
    "#                         \"mi_loss_weight\" : 0.001,\n",
    "#                         \"mi_loss\" : False,\n",
    "#                         \"alpha\" : 1,\n",
    "#                         \"beta_c\" : 0.05,\n",
    "#                         \"beta_d\" : 0.05,\n",
    "#                         \"warmup_ratio\" : 0.2,\n",
    "#                         \"weight_decay\" : 0.001,\n",
    "#                         \"target_loss_coeff\" : 1,\n",
    "#                         \"hate_coeff\" : 0,\n",
    "#                         \"fname\" : \"_VAE_with_rob_base_NoHL_balancedData.pt\",\n",
    "#                     }\n",
    "            \n",
    "#             params1 = SimpleNamespace(**params1)\n",
    "#             # enc_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "#             enc_model_name = \"roberta-base\"\n",
    "#             dec_model_name = \"facebook/bart-base\"\n",
    "#             # enc_chk = \"./FineTuned/Roberta/pytorch_model.bin\"\n",
    "#             print(enc_files[f])\n",
    "#             enc_chk = enc_files[f]#\"./FineTuned/RobertaHate/pytorch_model.bin\"\n",
    "#             dec_chk = dec_files[0]#\"./FineTuned/BART/pytorch_model.bin\"\n",
    "#             hate_model = CATCH(1.0,enc_chk,enc_model_name,dec_chk,dec_model_name, params1.num_targets,params1.num_labels,params1.beta_c, params1.beta_d,device,params1.train_batch_size,params1.decoder_type).to(device)\n",
    "#             encoder_tokenizer = RobertaTokenizer.from_pretrained(enc_model_name)\n",
    "#             decoder_tokenizer = BartTokenizer.from_pretrained(dec_model_name)\n",
    "#             print(params1.dataset_name)\n",
    "#             print(files[f])\n",
    "#             print(train_frame.shape)\n",
    "#             wandb.watch(hate_model)\n",
    "#             train(hate_model=hate_model,\n",
    "#             train_data=train_frame['text'].values.tolist(), \n",
    "#             train_labels=train_frame['label'].values.tolist(), \n",
    "#             train_target=train_frame['target'].values.tolist(), \n",
    "#             val_data=test_frame['text'].values.tolist(), \n",
    "#             val_labels=test_frame['final_label'].values.tolist(), \n",
    "#             val_target = test_frame['target'].values.tolist(),\n",
    "#             encoder_tokenizer=encoder_tokenizer,\n",
    "#             decoder_tokenizer=decoder_tokenizer,\n",
    "#             params1=params1)\n",
    "#             # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# import pandas as pd\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from types import SimpleNamespace\n",
    "\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# files = ['./Preprocessed Datasets/GAB/gab_HX_Numeric_train.csv',\n",
    "#            './Preprocessed Datasets/Reddit/reddit_TRY_Numeric_train.csv',\n",
    "#            './Preprocessed Datasets/Twitter/twitter_HX_Numeric_train.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_train.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_Numeric_train.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_Numeric_train.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_train.csv',\n",
    "#             './Preprocessed Datasets/YouTube/youtube_IC_Numeric_balanced.csv']\n",
    "\n",
    "# test_files = ['./Preprocessed Datasets/GAB/gab_HX_Numeric_test.csv',\n",
    "#            './Preprocessed Datasets/Reddit/reddit_TRY_Numeric_test.csv',\n",
    "#            './Preprocessed Datasets/Twitter/twitter_HX_Numeric_test.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_test.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_Numeric_test.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_Numeric_test.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_test.csv',\n",
    "#             './Preprocessed Datasets/Youtube/youtube_IC_Numeric_test.csv']\n",
    "\n",
    "# enc_files = ['./FineTuned/Roberta/gab_HX/pytorch_model.bin',\n",
    "#             './FineTuned/Roberta/reddit_TRY/pytorch_model.bin',\n",
    "#             './FineTuned/Roberta/twitter_HX/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/twitter_TRY/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/twitter/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/youtube/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/youtube_TRY/pytorch_model.bin',\n",
    "#             './FineTuned/Roberta/youtube_IC/pytorch_model.bin']\n",
    "\n",
    "# # enc_files = ['./FineTuned/Roberta/gab/pytorch_model.bin',\n",
    "# #              './FineTuned/RobertaHate/reddit_TRY/pytorch_model.bin',\n",
    "# #              './FineTuned/RobertaHate/twitter/pytorch_model.bin',\n",
    "# #              './FineTuned/RobertaHate/youtube/pytorch_model.bin']\n",
    "\n",
    "# dec_files = ['./FineTuned/BART/pytorch_model.bin',\n",
    "#              './FineTuned/BART_reddit_TRY/checkpoint-7000/pytorch_model.bin',\n",
    "#              './FineTuned/BART_twitter/pytorch_model.bin',\n",
    "#              './FineTuned/BART_youtube/pytorch_model.bin']\n",
    "\n",
    "\n",
    "# dataset_names = [\"gab_HX\",\"reddit_TRY\",\"twitter_HX\",\"youtube_IC\"]#\"twitter_TRY\",\"twitter\",\"youtube\",\"youtube_TRY\",\n",
    "# hidden_size = 768\n",
    "# classifier_dropout = 0.2\n",
    "# learning_rate = 5e-3\n",
    "# print(files)\n",
    "# print(len(files))\n",
    "# print(device)\n",
    "# filenames = set()\n",
    "# latent_variables = ['hate','target']#['0','1','2','3','4','5','6','7']\n",
    "\n",
    "# for f in range(1,len(files)):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         train_frame = pd.read_csv(files[f])\n",
    "#         tar = np.zeros(8)\n",
    "#         a1 = np.unique(train_frame['target'])\n",
    "#         class_weights = compute_class_weight('balanced', classes=a1, y=train_frame['target'])\n",
    "#         for i in range(len(a1)):\n",
    "#             try:\n",
    "#                 tar[a1[i]] = class_weights[a1[i]]\n",
    "#             except:\n",
    "#                 tar[a1[i]] = 0\n",
    "#         print(tar)\n",
    "#         class_weights2 = torch.FloatTensor(tar).to(device)\n",
    "#         print(class_weights2)\n",
    "#         if(dataset_names[f] not in filenames):\n",
    "#             filenames.add(dataset_names[f])\n",
    "#             # train_frame = pd.read_csv(files[f])\n",
    "#             if(f+1<len(files)):\n",
    "#                 print(\"TEST FILE: \",test_files[f+1])\n",
    "#                 test_frame = pd.read_csv(test_files[f+1])\n",
    "#             else:\n",
    "#                  test_frame = pd.read_csv(test_files[0])\n",
    "#             class_weights1 = compute_class_weight('balanced', classes=np.unique(train_frame['label']), y=train_frame['label'])\n",
    "#             class_weights1 = torch.FloatTensor(class_weights1)\n",
    "#             x = train_frame['label'].value_counts().values\n",
    "#             class_weights3 = torch.FloatTensor([x[0]/sum(x),x[1]/sum(x)])\n",
    "#             # class_weights3 = torch.FloatTensor([0.6,0.4])\n",
    "#             h_weights = class_weights3[train_frame['label']]\n",
    "#             print(h_weights)\n",
    "#             class_weights1 = class_weights1.to(device)\n",
    "#             wandb.init(\n",
    "#                 # Set the project where this run will be logged\n",
    "#                 project=\"Disentangling Hate Speech and Target\", \n",
    "#                 # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "#                 name=f\"experiment_ROB_BART_{dataset_names[f]}_{classifier_dropout}_{learning_rate}\",\n",
    "#                 # Track hyperparameters and run metadata\n",
    "#                 config={\n",
    "#                         \"max_sequence_length\": 512, \n",
    "#                         \"train_batch_size\" : 8, \n",
    "#                         \"val_batch_size\" : 8,\n",
    "#                         \"hidden_dim\" : 512, \n",
    "#                         \"hate_dim\" : 384,\n",
    "#                         \"num_epochs\" : 100, \n",
    "#                         \"device\" : device,\n",
    "#                         \"dataset_name\" : dataset_names[f],\n",
    "#                         \"h_weights\" : h_weights,\n",
    "#                         \"hate_class_weights\" : class_weights1,\n",
    "#                         \"target_class_weights\" : class_weights2,\n",
    "#                         \"hidden_size\" : 768,\n",
    "#                         \"num_labels\": 2,\n",
    "#                         \"num_targets\": 8,\n",
    "#                         \"classifier_dropout\" : classifier_dropout,\n",
    "#                         \"content_lr\": 1e-4,\n",
    "#                         \"decoder_type\" : \"BART\",\n",
    "#                         \"kl_weight\" : 0.05,\n",
    "#                         \"mi_loss_weight\" : 0.001,\n",
    "#                         \"mi_loss\" : False,\n",
    "#                         \"alpha\" : 1,\n",
    "#                         \"beta_c\" : 0.05,\n",
    "#                         \"beta_d\" : 0.05,\n",
    "#                         \"warmup_ratio\" : 0.2,\n",
    "#                         \"weight_decay\" : 0.001,\n",
    "#                         \"target_loss_coeff\" : 0,\n",
    "#                         \"hate_coeff\" : 1,\n",
    "#                         \"fname\" : \"_VAE_with_rob_base_NoTL_balancedData.pt\",\n",
    "#                         })\n",
    "#             params1 = { \n",
    "#                         \"max_sequence_length\": 512, \n",
    "#                         \"train_batch_size\" : 8, \n",
    "#                         \"val_batch_size\" : 8,\n",
    "#                         \"hidden_dim\" : 512, \n",
    "#                         \"hate_dim\" : 384,\n",
    "#                         \"num_epochs\" : 100, \n",
    "#                         \"device\" : device,\n",
    "#                         \"dataset_name\" : dataset_names[f],\n",
    "#                         \"h_weights\" : h_weights,\n",
    "#                         \"hate_class_weights\" : class_weights1,\n",
    "#                         \"target_class_weights\" : class_weights2,\n",
    "#                         \"hidden_size\" : 768,\n",
    "#                         \"num_labels\": 2,\n",
    "#                         \"num_targets\": 8,\n",
    "#                         \"classifier_dropout\" : classifier_dropout,\n",
    "#                         \"content_lr\": 1e-4,\n",
    "#                         \"decoder_type\" : \"BART\",\n",
    "#                         \"kl_weight\" : 0.05,\n",
    "#                         \"mi_loss_weight\" : 0.001,\n",
    "#                         \"mi_loss\" : False,\n",
    "#                         \"alpha\" : 1,\n",
    "#                         \"beta_c\" : 0.05,\n",
    "#                         \"beta_d\" : 0.05,\n",
    "#                         \"warmup_ratio\" : 0.2,\n",
    "#                         \"weight_decay\" : 0.001,\n",
    "#                         \"target_loss_coeff\" : 0,\n",
    "#                         \"hate_coeff\" : 1,\n",
    "#                         \"fname\" : \"_VAE_with_rob_base_NoTL_balancedData.pt\",\n",
    "#                     }\n",
    "            \n",
    "#             params1 = SimpleNamespace(**params1)\n",
    "#             # enc_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "#             enc_model_name = \"roberta-base\"\n",
    "#             dec_model_name = \"facebook/bart-base\"\n",
    "#             # enc_chk = \"./FineTuned/Roberta/pytorch_model.bin\"\n",
    "#             print(enc_files[f])\n",
    "#             enc_chk = enc_files[f]#\"./FineTuned/RobertaHate/pytorch_model.bin\"\n",
    "#             dec_chk = dec_files[f]#\"./FineTuned/BART/pytorch_model.bin\"\n",
    "#             hate_model = CATCH(1.0,enc_chk,enc_model_name,dec_chk,dec_model_name, params1.num_targets,params1.num_labels,params1.beta_c, params1.beta_d,device,params1.train_batch_size,params1.decoder_type).to(device)\n",
    "#             encoder_tokenizer = RobertaTokenizer.from_pretrained(enc_model_name)\n",
    "#             decoder_tokenizer = BartTokenizer.from_pretrained(dec_model_name)\n",
    "#             print(params1.dataset_name)\n",
    "#             print(files[f])\n",
    "#             print(train_frame.shape)\n",
    "#             wandb.watch(hate_model)\n",
    "#             train(hate_model=hate_model,\n",
    "#             train_data=train_frame['text'].values.tolist(), \n",
    "#             train_labels=train_frame['label'].values.tolist(), \n",
    "#             train_target=train_frame['target'].values.tolist(), \n",
    "#             val_data=test_frame['text'].values.tolist(), \n",
    "#             val_labels=test_frame['final_label'].values.tolist(), \n",
    "#             val_target = test_frame['target'].values.tolist(),\n",
    "#             encoder_tokenizer=encoder_tokenizer,\n",
    "#             decoder_tokenizer=decoder_tokenizer,\n",
    "#             params1=params1)\n",
    "#             # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# import pandas as pd\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from types import SimpleNamespace\n",
    "\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# files = ['./Preprocessed Datasets/GAB/gab_HX_Numeric_train.csv',\n",
    "#            './Preprocessed Datasets/Reddit/reddit_TRY_Numeric_train.csv',\n",
    "#            './Preprocessed Datasets/Twitter/twitter_HX_Numeric_train.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_train.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_Numeric_train.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_Numeric_train.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_train.csv',\n",
    "#             './Preprocessed Datasets/YouTube/youtube_IC_Numeric_balanced.csv']\n",
    "\n",
    "# test_files = ['./Preprocessed Datasets/GAB/gab_HX_Numeric_test.csv',\n",
    "#            './Preprocessed Datasets/Reddit/reddit_TRY_Numeric_test.csv',\n",
    "#            './Preprocessed Datasets/Twitter/twitter_HX_Numeric_test.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_test.csv',\n",
    "#         #    './Preprocessed Datasets/Twitter/twitter_Numeric_test.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_Numeric_test.csv',\n",
    "#         #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_test.csv',\n",
    "#             './Preprocessed Datasets/Youtube/youtube_IC_Numeric_test.csv']\n",
    "\n",
    "# enc_files = ['./FineTuned/Roberta/gab_HX/pytorch_model.bin',\n",
    "#             './FineTuned/Roberta/reddit_TRY/pytorch_model.bin',\n",
    "#             './FineTuned/Roberta/twitter_HX/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/twitter_TRY/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/twitter/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/youtube/pytorch_model.bin',\n",
    "#             # './FineTuned/Roberta/youtube_TRY/pytorch_model.bin',\n",
    "#             './FineTuned/Roberta/youtube_IC/pytorch_model.bin']\n",
    "\n",
    "# # enc_files = ['./FineTuned/Roberta/gab/pytorch_model.bin',\n",
    "# #              './FineTuned/RobertaHate/reddit_TRY/pytorch_model.bin',\n",
    "# #              './FineTuned/RobertaHate/twitter/pytorch_model.bin',\n",
    "# #              './FineTuned/RobertaHate/youtube/pytorch_model.bin']\n",
    "\n",
    "# dec_files = ['./FineTuned/BART/pytorch_model.bin',\n",
    "#              './FineTuned/BART_reddit_TRY/checkpoint-7000/pytorch_model.bin',\n",
    "#              './FineTuned/BART_twitter/pytorch_model.bin',\n",
    "#              './FineTuned/BART_youtube/pytorch_model.bin']\n",
    "\n",
    "\n",
    "# dataset_names = [\"gab_HX\",\"reddit_TRY\",\"twitter_HX\",\"youtube_IC\"]#\"twitter_TRY\",\"twitter\",\"youtube\",\"youtube_TRY\",\n",
    "# hidden_size = 768\n",
    "# classifier_dropout = 0.2\n",
    "# learning_rate = 5e-3\n",
    "# print(files)\n",
    "# print(len(files))\n",
    "# print(device)\n",
    "# filenames = set()\n",
    "# latent_variables = ['hate','target']#['0','1','2','3','4','5','6','7']\n",
    "\n",
    "# for f in range(0,len(files)):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         train_frame = pd.read_csv(files[f])\n",
    "#         tar = np.zeros(8)\n",
    "#         a1 = np.unique(train_frame['target'])\n",
    "#         class_weights = compute_class_weight('balanced', classes=a1, y=train_frame['target'])\n",
    "#         for i in range(len(a1)):\n",
    "#             try:\n",
    "#                 tar[a1[i]] = class_weights[a1[i]]\n",
    "#             except:\n",
    "#                 tar[a1[i]] = 0\n",
    "#         print(tar)\n",
    "#         class_weights2 = torch.FloatTensor(tar).to(device)\n",
    "#         print(class_weights2)\n",
    "#         if(dataset_names[f] not in filenames):\n",
    "#             filenames.add(dataset_names[f])\n",
    "#             # train_frame = pd.read_csv(files[f])\n",
    "#             if(f+1<len(files)):\n",
    "#                 print(\"TEST FILE: \",test_files[f+1])\n",
    "#                 test_frame = pd.read_csv(test_files[f+1])\n",
    "#             else:\n",
    "#                  test_frame = pd.read_csv(test_files[0])\n",
    "#             class_weights1 = compute_class_weight('balanced', classes=np.unique(train_frame['label']), y=train_frame['label'])\n",
    "#             class_weights1 = torch.FloatTensor(class_weights1)\n",
    "#             x = train_frame['label'].value_counts().values\n",
    "#             class_weights3 = torch.FloatTensor([x[0]/sum(x),x[1]/sum(x)])\n",
    "#             # class_weights3 = torch.FloatTensor([0.6,0.4])\n",
    "#             h_weights = class_weights3[train_frame['label']]\n",
    "#             print(h_weights)\n",
    "#             class_weights1 = class_weights1.to(device)\n",
    "#             wandb.init(\n",
    "#                 # Set the project where this run will be logged\n",
    "#                 project=\"Disentangling Hate Speech and Target\", \n",
    "#                 # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "#                 name=f\"experiment_ROB_BART_{dataset_names[f]}_{classifier_dropout}_{learning_rate}\",\n",
    "#                 # Track hyperparameters and run metadata\n",
    "#                 config={\n",
    "#                         \"max_sequence_length\": 512, \n",
    "#                         \"train_batch_size\" : 8, \n",
    "#                         \"val_batch_size\" : 8,\n",
    "#                         \"hidden_dim\" : 512, \n",
    "#                         \"hate_dim\" : 384,\n",
    "#                         \"num_epochs\" : 100, \n",
    "#                         \"device\" : device,\n",
    "#                         \"dataset_name\" : dataset_names[f],\n",
    "#                         \"h_weights\" : h_weights,\n",
    "#                         \"hate_class_weights\" : class_weights1,\n",
    "#                         \"target_class_weights\" : class_weights2,\n",
    "#                         \"hidden_size\" : 768,\n",
    "#                         \"num_labels\": 2,\n",
    "#                         \"num_targets\": 8,\n",
    "#                         \"classifier_dropout\" : classifier_dropout,\n",
    "#                         \"content_lr\": 1e-4,\n",
    "#                         \"decoder_type\" : \"BART\",\n",
    "#                         \"kl_weight\" : 0.05,\n",
    "#                         \"mi_loss_weight\" : 0.001,\n",
    "#                         \"mi_loss\" : False,\n",
    "#                         \"alpha\" : 1,\n",
    "#                         \"beta_c\" : 0.05,\n",
    "#                         \"beta_d\" : 0.05,\n",
    "#                         \"warmup_ratio\" : 0.2,\n",
    "#                         \"weight_decay\" : 0.001,\n",
    "#                         \"target_loss_coeff\" : 0,\n",
    "#                         \"hate_coeff\" : 0,\n",
    "#                         \"fname\" : \"_VAE_with_rob_base_NoBL_balancedData.pt\",\n",
    "#                         })\n",
    "#             params1 = { \n",
    "#                         \"max_sequence_length\": 512, \n",
    "#                         \"train_batch_size\" : 8, \n",
    "#                         \"val_batch_size\" : 8,\n",
    "#                         \"hidden_dim\" : 512, \n",
    "#                         \"hate_dim\" : 384,\n",
    "#                         \"num_epochs\" : 100, \n",
    "#                         \"device\" : device,\n",
    "#                         \"dataset_name\" : dataset_names[f],\n",
    "#                         \"h_weights\" : h_weights,\n",
    "#                         \"hate_class_weights\" : class_weights1,\n",
    "#                         \"target_class_weights\" : class_weights2,\n",
    "#                         \"hidden_size\" : 768,\n",
    "#                         \"num_labels\": 2,\n",
    "#                         \"num_targets\": 8,\n",
    "#                         \"classifier_dropout\" : classifier_dropout,\n",
    "#                         \"content_lr\": 1e-4,\n",
    "#                         \"decoder_type\" : \"BART\",\n",
    "#                         \"kl_weight\" : 0.05,\n",
    "#                         \"mi_loss_weight\" : 0.001,\n",
    "#                         \"mi_loss\" : False,\n",
    "#                         \"alpha\" : 1,\n",
    "#                         \"beta_c\" : 0.05,\n",
    "#                         \"beta_d\" : 0.05,\n",
    "#                         \"warmup_ratio\" : 0.2,\n",
    "#                         \"weight_decay\" : 0.001,\n",
    "#                         \"target_loss_coeff\" : 0,\n",
    "#                         \"hate_coeff\" : 0,\n",
    "#                         \"fname\" : \"_VAE_with_rob_base_NoBL_balancedData.pt\",\n",
    "#                     }\n",
    "            \n",
    "#             params1 = SimpleNamespace(**params1)\n",
    "#             # enc_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "#             enc_model_name = \"roberta-base\"\n",
    "#             dec_model_name = \"facebook/bart-base\"\n",
    "#             # enc_chk = \"./FineTuned/Roberta/pytorch_model.bin\"\n",
    "#             print(enc_files[f])\n",
    "#             enc_chk = enc_files[f]#\"./FineTuned/RobertaHate/pytorch_model.bin\"\n",
    "#             dec_chk = dec_files[0]#\"./FineTuned/BART/pytorch_model.bin\"\n",
    "#             hate_model = CATCH(1.0,enc_chk,enc_model_name,dec_chk,dec_model_name, params1.num_targets,params1.num_labels,params1.beta_c, params1.beta_d,device,params1.train_batch_size,params1.decoder_type).to(device)\n",
    "#             encoder_tokenizer = RobertaTokenizer.from_pretrained(enc_model_name)\n",
    "#             decoder_tokenizer = BartTokenizer.from_pretrained(dec_model_name)\n",
    "#             print(params1.dataset_name)\n",
    "#             print(files[f])\n",
    "#             print(train_frame.shape)\n",
    "#             wandb.watch(hate_model)\n",
    "#             train(hate_model=hate_model,\n",
    "#             train_data=train_frame['text'].values.tolist(), \n",
    "#             train_labels=train_frame['label'].values.tolist(), \n",
    "#             train_target=train_frame['target'].values.tolist(), \n",
    "#             val_data=test_frame['text'].values.tolist(), \n",
    "#             val_labels=test_frame['final_label'].values.tolist(), \n",
    "#             val_target = test_frame['target'].values.tolist(),\n",
    "#             encoder_tokenizer=encoder_tokenizer,\n",
    "#             decoder_tokenizer=decoder_tokenizer,\n",
    "#             params1=params1)\n",
    "#             # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data, test_labels, target_labels, encoder_tokenizer, decoder_tokenizer, params1):\n",
    "    \n",
    "    test = EncodedDataset(input_sents=test_data, \n",
    "                    input_labels=test_labels, \n",
    "                    target_labels=target_labels,  \n",
    "                    encoder_tokenizer=encoder_tokenizer,\n",
    "                    decoder_tokenizer=decoder_tokenizer,\n",
    "                    max_sequence_length=params1.max_sequence_length)\n",
    "    \n",
    "\n",
    "    val_dataloader = DataLoader(test, batch_size=params1.val_batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    predictions = []\n",
    "    y_true = []\n",
    "    total_acc_val = 0\n",
    "    total_acc_target_val = 0\n",
    "    hate_model.eval()\n",
    "    e_cnt=0\n",
    "    with torch.no_grad():\n",
    "      for val_input, val_mask, val_target, val_label, val_dec_input, val_dec_mask in val_dataloader:\n",
    "        e_cnt+=1\n",
    "        val_input = val_input.to(device)\n",
    "        val_mask = val_mask.to(device)\n",
    "        val_target = val_target.to(device)\n",
    "        val_label = val_label.to(device)\n",
    "        val_dec_input = val_dec_input.to(device)\n",
    "        val_dec_mask = val_dec_mask.to(device)\n",
    "        elbo_loss_l, hate_logits, target_logits = hate_model(val_input, val_mask, val_dec_input, val_dec_mask, val_input,val_target, val_label)\n",
    "        total_acc_val += round(accuracy_score(torch.argmax(val_label, dim=1).cpu().numpy().tolist(), torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "        total_acc_target_val += round(accuracy_score(torch.argmax(val_target, dim=1).cpu().numpy().tolist(), torch.argmax(target_logits,dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "        y_true += torch.argmax(val_label, dim=1).cpu().numpy().tolist()\n",
    "        predictions += torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()\n",
    "      print(classification_report(y_true, predictions))\n",
    "      print(\"Accuracy: \", total_acc_val/e_cnt)\n",
    "      print(\"Target Accuracy: \", total_acc_target_val/e_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_gab_HX_13_VAE_with_rob_base_ft_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.71      0.77       798\n",
      "           1       0.48      0.68      0.56       314\n",
      "\n",
      "    accuracy                           0.70      1112\n",
      "   macro avg       0.67      0.70      0.67      1112\n",
      "weighted avg       0.75      0.70      0.71      1112\n",
      "\n",
      "Accuracy:  70.23381294964028\n",
      "Target Accuracy:  13.848920863309353\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_gab_HX_6_VAE_with_rob_base_NoHL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.39      0.49       798\n",
      "           1       0.24      0.48      0.32       314\n",
      "\n",
      "    accuracy                           0.42      1112\n",
      "   macro avg       0.45      0.44      0.40      1112\n",
      "weighted avg       0.54      0.42      0.44      1112\n",
      "\n",
      "Accuracy:  41.54676258992806\n",
      "Target Accuracy:  15.107913669064748\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_gab_HX_6_VAE_with_rob_base_NoBL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.40      0.51       798\n",
      "           1       0.25      0.51      0.34       314\n",
      "\n",
      "    accuracy                           0.43      1112\n",
      "   macro avg       0.46      0.46      0.42      1112\n",
      "weighted avg       0.56      0.43      0.46      1112\n",
      "\n",
      "Accuracy:  43.34532374100719\n",
      "Target Accuracy:  10.881294964028777\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_2_VAE_with_rob_base_ft_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.66      0.78       798\n",
      "           1       0.52      0.95      0.67       314\n",
      "\n",
      "    accuracy                           0.74      1112\n",
      "   macro avg       0.75      0.81      0.73      1112\n",
      "weighted avg       0.85      0.74      0.75      1112\n",
      "\n",
      "Accuracy:  74.10071942446044\n",
      "Target Accuracy:  13.399280575539569\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_1_VAE_with_rob_base_NoHL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.57      0.64       798\n",
      "           1       0.30      0.47      0.37       314\n",
      "\n",
      "    accuracy                           0.54      1112\n",
      "   macro avg       0.52      0.52      0.51      1112\n",
      "weighted avg       0.61      0.54      0.57      1112\n",
      "\n",
      "Accuracy:  54.49640287769784\n",
      "Target Accuracy:  13.03956834532374\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_3_VAE_with_rob_base_NoTL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.72      0.82       798\n",
      "           1       0.56      0.91      0.70       314\n",
      "\n",
      "    accuracy                           0.78      1112\n",
      "   macro avg       0.76      0.82      0.76      1112\n",
      "weighted avg       0.84      0.78      0.79      1112\n",
      "\n",
      "Accuracy:  77.5179856115108\n",
      "Target Accuracy:  12.949640287769784\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_1_VAE_with_rob_base_NoBL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.55      0.62       798\n",
      "           1       0.28      0.46      0.35       314\n",
      "\n",
      "    accuracy                           0.52      1112\n",
      "   macro avg       0.50      0.50      0.49      1112\n",
      "weighted avg       0.60      0.52      0.55      1112\n",
      "\n",
      "Accuracy:  52.33812949640288\n",
      "Target Accuracy:  10.701438848920864\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_twitter_HX_6_VAE_with_rob_base_ft_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.55      0.68       798\n",
      "           1       0.43      0.86      0.57       314\n",
      "\n",
      "    accuracy                           0.63      1112\n",
      "   macro avg       0.67      0.70      0.63      1112\n",
      "weighted avg       0.77      0.63      0.65      1112\n",
      "\n",
      "Accuracy:  63.489208633093526\n",
      "Target Accuracy:  13.93884892086331\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_twitter_HX_3_VAE_with_rob_base_NoHL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.55      0.62       798\n",
      "           1       0.29      0.48      0.36       314\n",
      "\n",
      "    accuracy                           0.53      1112\n",
      "   macro avg       0.51      0.51      0.49      1112\n",
      "weighted avg       0.60      0.53      0.55      1112\n",
      "\n",
      "Accuracy:  52.60791366906475\n",
      "Target Accuracy:  11.870503597122303\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_twitter_HX_11_VAE_with_rob_base_NoTL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.60      0.71       798\n",
      "           1       0.44      0.80      0.57       314\n",
      "\n",
      "    accuracy                           0.66      1112\n",
      "   macro avg       0.66      0.70      0.64      1112\n",
      "weighted avg       0.76      0.66      0.67      1112\n",
      "\n",
      "Accuracy:  65.55755395683454\n",
      "Target Accuracy:  11.241007194244604\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_twitter_HX_3_VAE_with_rob_base_NoBL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.56      0.64       798\n",
      "           1       0.31      0.49      0.38       314\n",
      "\n",
      "    accuracy                           0.54      1112\n",
      "   macro avg       0.52      0.53      0.51      1112\n",
      "weighted avg       0.62      0.54      0.56      1112\n",
      "\n",
      "Accuracy:  54.226618705035975\n",
      "Target Accuracy:  11.690647482014388\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_youtube_IC_10_VAE_with_rob_base_ft_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.68      0.77       798\n",
      "           1       0.49      0.76      0.60       314\n",
      "\n",
      "    accuracy                           0.71      1112\n",
      "   macro avg       0.68      0.72      0.68      1112\n",
      "weighted avg       0.77      0.71      0.72      1112\n",
      "\n",
      "Accuracy:  70.68345323741008\n",
      "Target Accuracy:  11.151079136690647\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_youtube_IC_2_VAE_with_rob_base_NoHL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.45      0.55       798\n",
      "           1       0.27      0.52      0.35       314\n",
      "\n",
      "    accuracy                           0.47      1112\n",
      "   macro avg       0.49      0.48      0.45      1112\n",
      "weighted avg       0.58      0.47      0.49      1112\n",
      "\n",
      "Accuracy:  46.76258992805755\n",
      "Target Accuracy:  11.06115107913669\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_youtube_IC_7_VAE_with_rob_base_NoTL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.57      0.67       798\n",
      "           1       0.38      0.68      0.49       314\n",
      "\n",
      "    accuracy                           0.60      1112\n",
      "   macro avg       0.60      0.62      0.58      1112\n",
      "weighted avg       0.70      0.60      0.62      1112\n",
      "\n",
      "Accuracy:  59.98201438848921\n",
      "Target Accuracy:  10.251798561151078\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/BuildingFramework/Models/Disentanglement/best-model_youtube_IC_2_VAE_with_rob_base_NoBL_balancedData.pt\n",
      "****************************************************************************************************\n",
      "C:/Users/psheth5/Downloads/Cross-Target Hate Speech Datasets/reddit_TRY_Numeric_test_subset.csv\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "model_files = [#'./BuildingFramework/Models/Disentanglement/best-model_gab_HX_1_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_gab_HX_2_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_gab_HX_3_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_gab_HX_4_VAE_with_rob_base_ft.pt',\n",
    "               './BuildingFramework/Models/Disentanglement/best-model_gab_HX_13_VAE_with_rob_base_ft_balancedData.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_gab_HX_19_VAE_with_rob_base_NoFT_balancedData.pt',\n",
    "               './BuildingFramework/Models/Disentanglement/best-model_gab_HX_6_VAE_with_rob_base_NoHL_balancedData.pt',\n",
    "               './BuildingFramework/Models/Disentanglement/best-model_gab_HX_6_VAE_with_rob_base_NoBL_balancedData.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_1_VAE_with_rob_base_ft.pt',\n",
    "               './BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_2_VAE_with_rob_base_ft_balancedData.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_3_VAE_with_rob_base_NoFT_balancedData.pt',\n",
    "               './BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_1_VAE_with_rob_base_NoHL_balancedData.pt',\n",
    "               './BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_3_VAE_with_rob_base_NoTL_balancedData.pt',\n",
    "               './BuildingFramework/Models/Disentanglement/best-model_reddit_TRY_1_VAE_with_rob_base_NoBL_balancedData.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_1_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_2_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_3_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_4_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_5_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_HX_6_VAE_with_rob_base_ft.pt',\n",
    "                './BuildingFramework/Models/Disentanglement/best-model_twitter_HX_6_VAE_with_rob_base_ft_balancedData.pt',\n",
    "              # './BuildingFramework/Models/Disentanglement/best-model_twitter_HX_7_VAE_with_rob_base_NoFT_balancedData.pt',\n",
    "                './BuildingFramework/Models/Disentanglement/best-model_twitter_HX_3_VAE_with_rob_base_NoHL_balancedData.pt',\n",
    "                './BuildingFramework/Models/Disentanglement/best-model_twitter_HX_11_VAE_with_rob_base_NoTL_balancedData.pt',\n",
    "                './BuildingFramework/Models/Disentanglement/best-model_twitter_HX_3_VAE_with_rob_base_NoBL_balancedData.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_TRY_1_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_Twitter_TRY_2_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_twitter_1_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_twitter_2_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_youtube_1_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_youtube_2_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_youtube_TRY_1_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_1_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_4_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_5_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_6_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_8_VAE_with_rob_base_ft.pt',\n",
    "               #'./BuildingFramework/Models/Disentanglement/best-model_youtube_IC_11_VAE_with_rob_base_ft.pt',\n",
    "                './BuildingFramework/Models/Disentanglement/best-model_youtube_IC_10_VAE_with_rob_base_ft_balancedData.pt',\n",
    "              # './BuildingFramework/Models/Disentanglement/best-model_youtube_IC_1_VAE_with_rob_base_NoFT_balancedData.pt',\n",
    "                './BuildingFramework/Models/Disentanglement/best-model_youtube_IC_2_VAE_with_rob_base_NoHL_balancedData.pt',\n",
    "                './BuildingFramework/Models/Disentanglement/best-model_youtube_IC_7_VAE_with_rob_base_NoTL_balancedData.pt',\n",
    "                './BuildingFramework/Models/Disentanglement/best-model_youtube_IC_2_VAE_with_rob_base_NoBL_balancedData.pt']\n",
    "\n",
    "test_files = ['./Preprocessed Datasets/GAB/gab_HX_Numeric_test.csv',\n",
    "           './Preprocessed Datasets/Reddit/reddit_TRY_Numeric_test.csv',\n",
    "           './Preprocessed Datasets/Twitter/twitter_HX_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_Numeric_test.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_Numeric_test.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_test.csv',\n",
    "            './Preprocessed Datasets/YouTube/youtube_IC_Numeric_test_balanced.csv']\n",
    "\n",
    "test_files = ['./reddit_TRY_Numeric_test_subset.csv']\n",
    "\n",
    "for f in model_files:\n",
    "        print(f)\n",
    "        print(\"*\"*100)\n",
    "        for f1 in range(0,len(test_files)):\n",
    "                print(test_files[f1])\n",
    "                print(\"*\"*100)\n",
    "                hate_model.load_state_dict(torch.load(f))\n",
    "                test_frame =  pd.read_csv(test_files[f1])\n",
    "                evaluate(model=hate_model, test_data = test_frame['text'].values.tolist(), test_labels=test_frame['final_label'].values.tolist(),target_labels=test_frame['target'].values.tolist(), encoder_tokenizer=encoder_tokenizer,decoder_tokenizer=decoder_tokenizer,params1=params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a6511c335c26d02572349af57853ebfcc20500c776be641a45dee87cf591594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
