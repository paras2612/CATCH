{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.28.16\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW, PreTrainedTokenizer\n",
    "from collections import namedtuple\n",
    "import json\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "#from model import BARTVAEClassifier, BARTDecoderClassifier, BARTVADVAEClassifier, RobertaClassifier\n",
    "#from utils import ErcTextDataset, get_num_classes, get_label_VAD, convert_label_to_VAD, save_latent_params, compute_VAD_pearson_correlation, replace_for_robust_eval\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import yaml\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report, \\\n",
    "    precision_recall_fscore_support, precision_score, recall_score\n",
    "import torch.cuda.amp.grad_scaler as grad_scaler\n",
    "import torch.cuda.amp.autocast_mode as autocast_mode\n",
    "from transformers.models.bart.modeling_bart import BartModel, BartDecoder\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softplus\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, RobertaForMaskedLM, AutoModel, AutoTokenizer, AutoConfig, BartTokenizer, BartConfig,RobertaForSequenceClassification, BartForSequenceClassification\n",
    "from transformers.models.bart.modeling_bart import BartLearnedPositionalEmbedding, BartEncoderLayer, BartDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class EncodedDataset(Dataset):\n",
    "\n",
    "  def __init__(self, input_sents: List[str], \n",
    "                input_labels: List[int], \n",
    "                target_labels:List[int], \n",
    "                encoder_tokenizer: PreTrainedTokenizer,\n",
    "                decoder_tokenizer: PreTrainedTokenizer,\n",
    "                max_sequence_length: int = None, \n",
    "                max_targets: int = 8):\n",
    "      \n",
    "    self.input_sents = input_sents\n",
    "    self.input_labels = input_labels\n",
    "    self.target_labels = target_labels\n",
    "    self.encoder_tokenizer = encoder_tokenizer\n",
    "    self.decoder_tokenizer = decoder_tokenizer\n",
    "    self.max_sequence_length = max_sequence_length\n",
    "    self.max_targets = max_targets\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_sents) \n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    text = self.input_sents[index]\n",
    "    label = self.input_labels[index]\n",
    "    labels = np.zeros(2)\n",
    "    labels[label] = 1\n",
    "    target = self.target_labels[index]\n",
    "    target_labels = np.zeros(8)\n",
    "    target_labels[target] = 1\n",
    "    target_labels = torch.tensor(target_labels)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    token = self.encoder_tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
    "\n",
    "    input_ids, mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
    "\n",
    "    if self.decoder_tokenizer.pad_token is None:\n",
    "      self.decoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    token = self.decoder_tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
    "\n",
    "    dec_input_ids, dec_mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
    "\n",
    "    return input_ids, mask_ids, target_labels, labels, dec_input_ids, dec_mask_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Inference(nn.Sequential):\n",
    "    def __init__(self, num_input_channels, latent_dim, disc_variable=True):\n",
    "        super(_Inference, self).__init__()\n",
    "        if disc_variable:\n",
    "            self.add_module('fc', nn.Linear(num_input_channels, num_input_channels//2))\n",
    "            self.add_module('relu', nn.ReLU())\n",
    "            self.add_module('fc2', nn.Linear(num_input_channels//2, latent_dim))\n",
    "            self.add_module('log_softmax', nn.LogSoftmax(dim=1))\n",
    "        else:\n",
    "            self.add_module('fc', nn.Linear(num_input_channels, latent_dim))\n",
    "\n",
    "class Sample(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(Sample, self).__init__()\n",
    "        self._temperature = temperature\n",
    "\n",
    "    def forward(self, norm_mean, norm_log_sigma, disc_log_alpha, disc_label=None, mixup=False, disc_label_mixup=None,\n",
    "                mixup_lam=None):\n",
    "        \"\"\"\n",
    "        :param norm_mean: mean parameter of continuous norm variable\n",
    "        :param norm_log_sigma: log sigma parameter of continuous norm variable\n",
    "        :param disc_log_alpha: log alpha parameter of discrete multinomial variable\n",
    "        :param disc_label: the ground truth label of discrete variable (not one-hot label)\n",
    "        :param mixup: if we do mixup\n",
    "        :param disc_label_mixup: the mixup target label\n",
    "        :param mixup_lam: the mixup lambda\n",
    "        :return: sampled latent variable\n",
    "        \"\"\"\n",
    "        batch_size = norm_mean.size(0)\n",
    "        latent_sample = list([])\n",
    "        latent_sample.append(self._sample_norm(norm_mean, norm_log_sigma))\n",
    "        if disc_label is not None:\n",
    "            disc_label = torch.argmax(disc_label, dim=1)            \n",
    "            # it means we have the real label, then we can use real label instead of sampling\n",
    "            # c: N*c onehot\n",
    "            if mixup:\n",
    "                c_a = torch.zeros(disc_log_alpha.size()).cuda()\n",
    "                c_a = c_a.scatter(1, disc_label.view(-1, 1), 1)\n",
    "                c_b = torch.zeros(disc_log_alpha.size()).cuda()\n",
    "                c_b = c_b.scatter(1, disc_label_mixup.view(-1, 1), 1)\n",
    "                c = mixup_lam * c_a + (1 - mixup_lam) * c_b\n",
    "            else:\n",
    "                disc_label = disc_label.long()\n",
    "                c = torch.zeros(disc_log_alpha.size()).cuda()\n",
    "                c = c.scatter(1, disc_label, 1)\n",
    "            latent_sample.append(c)\n",
    "        else:\n",
    "            latent_sample.append(self._sample_gumbel_softmax(disc_log_alpha))\n",
    "        latent_sample = torch.cat(latent_sample, dim=1)\n",
    "        dim_size = latent_sample.size(1)\n",
    "        latent_sample = latent_sample.view(batch_size, dim_size, 1, 1)\n",
    "        return latent_sample\n",
    "\n",
    "    def _sample_gumbel_softmax(self, log_alpha):\n",
    "        \"\"\"\n",
    "        Samples from a gumbel-softmax distribution using the reparameterization\n",
    "        trick.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        log_alpha : torch.Tensor\n",
    "            Parameters of the gumbel-softmax distribution. Shape (N, D)\n",
    "        \"\"\"\n",
    "        EPS = 1e-12\n",
    "        unif = torch.rand(log_alpha.size()).cuda()\n",
    "        gumbel = -torch.log(-torch.log(unif + EPS) + EPS)\n",
    "        # Reparameterize to create gumbel softmax sample\n",
    "        logit = (log_alpha + gumbel) / self._temperature\n",
    "        return torch.softmax(logit, dim=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sample_norm(mu, log_sigma):\n",
    "        \"\"\"\n",
    "        :param mu: the mu for sampling with N*D\n",
    "        :param log_sigma: the log_sigma for sampling with N*D\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        std_z = torch.randn(mu.size())\n",
    "        if mu.is_cuda:\n",
    "            std_z = std_z.cuda()\n",
    "\n",
    "        return mu + torch.exp(log_sigma) * std_z\n",
    "\n",
    "\n",
    "\n",
    "class CATCH(nn.Module):\n",
    "    \"\"\"The VAD-VAE model.\"\"\"\n",
    "    def __init__(self, temperature,enc_chk,check_point, dec_chk,bart_check_point,num_targets, num_class, beta_c,beta_d, device, batch_size, decoder_type, x_sigma=1):\n",
    "        super(CATCH, self).__init__()\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.decoder_type = decoder_type\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature, requires_grad=True, device=device))\n",
    "\n",
    "        #Prepare the encoder and decoder for VAE.\n",
    "        # self.encoder = RobertaForSequenceClassification.from_pretrained(check_point).to(device)\n",
    "        if(enc_chk!= \"\"):\n",
    "            self.encoder = RobertaForSequenceClassification.from_pretrained(check_point).to(device)\n",
    "            self.encoder.load_state_dict(torch.load(enc_chk))\n",
    "            self.encoder = self.encoder.roberta\n",
    "        else:\n",
    "            self.encoder = RobertaModel.from_pretrained(check_point).to(device)\n",
    "        for param in self.encoder.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(check_point)\n",
    "        self.config = AutoConfig.from_pretrained(check_point)\n",
    "\n",
    "        self.cmi = torch.nn.Parameter(torch.rand(1, requires_grad=True, device=device))\n",
    "        self.dmi = torch.nn.Parameter(torch.rand(1, requires_grad=True, device=device))\n",
    "\n",
    "        hidden_size = self.config.hidden_size\n",
    "        if decoder_type == 'BART':\n",
    "            self.decoder = BartDecoder.from_pretrained(bart_check_point).to(device)\n",
    "            # self.decoder = BartForSequenceClassification.from_pretrained(bart_check_point).to(device)\n",
    "            # self.decoder.load_state_dict(torch.load(dec_chk))\n",
    "            # self.decoder = self.decoder.model.decoder\n",
    "        else:\n",
    "            self.decoder = nn.LSTM(hidden_size, hidden_size, 1, batch_first=True)\n",
    "\n",
    "        self.decoder_start_token_id = 2\n",
    "        self.lm_head = nn.Linear(hidden_size, self.config.vocab_size)\n",
    "        self.lm_loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.x_sigma = x_sigma\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout_rate = params1.classifier_dropout\n",
    "\n",
    "        #Prepare the disentanglement modules.\n",
    "        self.continuous_inference = nn.Sequential()\n",
    "        self.disc_latent_inference = nn.Sequential()\n",
    "        conti_mean_inf_module = _Inference(num_input_channels=hidden_size,\n",
    "                                           latent_dim=hidden_size-num_targets,\n",
    "                                           disc_variable=False)\n",
    "        conti_logsigma_inf_module = _Inference(num_input_channels=hidden_size,\n",
    "                                               latent_dim=hidden_size-num_targets,\n",
    "                                               disc_variable=False)\n",
    "        self.continuous_inference.add_module(\"mean\", conti_mean_inf_module)\n",
    "        self.continuous_inference.add_module(\"log_sigma\", conti_logsigma_inf_module)\n",
    "\n",
    "        self._disc_latent_dim = num_targets\n",
    "        dic_inf = _Inference(num_input_channels=hidden_size, latent_dim=self._disc_latent_dim,\n",
    "                             disc_variable=True)\n",
    "        self.disc_latent_inference = dic_inf\n",
    "        sample = Sample(temperature=self.temperature)\n",
    "        self.sample = sample\n",
    "\n",
    "        self.kl_beta_c = beta_c\n",
    "        self.kl_beta_d = beta_d\n",
    "\n",
    "        self.disc_log_prior_param = torch.log(\n",
    "            torch.tensor([1 / self._disc_latent_dim for i in range(self._disc_latent_dim)]).view(1, -1).float().cuda())\n",
    "\n",
    "\n",
    "        #Reconstructor\n",
    "        self.reconstructor = nn.Linear(hidden_size, hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = hidden_size)\n",
    "        #Hate Classifier\n",
    "        self.hate_classifier = nn.Sequential(\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size-num_targets,hidden_size-num_targets),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(hidden_size-num_targets, num_class),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        #Target Classifier\n",
    "        self.target_classifier = nn.Sequential(\n",
    "            # nn.Dropout(params1.classifier_dropout),\n",
    "            nn.Linear(num_targets,num_targets),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def get_lm_loss(self, logits, labels, masks):\n",
    "        '''Get the utterance reconstruction loss.'''\n",
    "        # labels = labels.float()\n",
    "        loss = self.lm_loss_fn(logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "        masked_loss = loss * masks.view(-1)\n",
    "        return torch.mean(masked_loss)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, mask, decoder_inputs, decoder_masks, decoder_labels, vad_labels, labels):\n",
    "        \"\"\"\n",
    "        :param inputs: The input of PLM. Dim: [B, seq_len]\n",
    "        :param mask: The mask for input x. Dim: [B, seq_len]\n",
    "        \"\"\"\n",
    "        '''decoder_input_ids = shift_tokens_right(\n",
    "            x, self.config.pad_token_id, self.decoder_start_token_id\n",
    "        )'''\n",
    "        x = self.encoder(inputs, attention_mask=mask)[0]\n",
    "        x = x[:, 0, :].squeeze(1)\n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        #Get the latent variables.\n",
    "        norm_mean = self.continuous_inference.mean(x)\n",
    "        norm_log_sigma = self.continuous_inference.log_sigma(x)\n",
    "        disc_log_alpha = self.disc_latent_inference(x)\n",
    "        latent_sample = self.sample(norm_mean, norm_log_sigma, disc_log_alpha)\n",
    "        \n",
    "        latent_sample = torch.squeeze(latent_sample)\n",
    "        decoder_hidden = self.reconstructor(latent_sample)\n",
    "        \n",
    "        if self.decoder_type == 'BART':\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_inputs,\n",
    "                attention_mask=decoder_masks,\n",
    "                encoder_hidden_states=decoder_hidden.unsqueeze(1))\n",
    "            lm_logits = self.lm_head(decoder_outputs.last_hidden_state)\n",
    "        else:\n",
    "            input_embeddings = self.encoder.embeddings(decoder_inputs)\n",
    "            h = decoder_hidden.unsqueeze(0)\n",
    "            decoder_outputs, (_, _) = self.decoder(input_embeddings, (h, torch.zeros(h.shape).to(self.device)))\n",
    "            lm_logits = self.lm_head(decoder_outputs)\n",
    "\n",
    "        # reconstruct_loss = F.mse_loss(lm_logits, x, reduction=\"mean\") / (2 * self.batch_size * (self.x_sigma ** 2))\n",
    "        # preds = F.gumbel_softmax(lm_logits, tau=1, hard=False)\n",
    "        reconstruct_loss = self.get_lm_loss(lm_logits, decoder_labels, decoder_masks)#/ (2 * self.batch_size * (self.x_sigma ** 2))\n",
    "\n",
    "\n",
    "        # calculate latent space KL divergence\n",
    "        z_mean_sq = norm_mean * norm_mean\n",
    "        z_log_sigma_sq = 2 * norm_log_sigma\n",
    "        z_sigma_sq = torch.exp(z_log_sigma_sq)\n",
    "        continuous_kl_loss = 0.5 * torch.sum(z_mean_sq + z_sigma_sq - z_log_sigma_sq - 1) / self.batch_size\n",
    "        # notice here we duplicate the 0.5 by each part\n",
    "        # disc param : log(a1),...,log(an) type\n",
    "        # disc_kl_loss = torch.sum(torch.exp(disc_log_alpha) * (disc_log_alpha - self.disc_log_prior_param)) / self.batch_size\n",
    "        \n",
    "        prior_kl_loss_l = self.kl_beta_c * torch.abs(continuous_kl_loss - self.cmi) #+ self.kl_beta_d * torch.abs(disc_kl_loss - self.dmi)\n",
    "        elbo_loss_l = reconstruct_loss + prior_kl_loss_l\n",
    "\n",
    "        #Calculate the classification loss.\n",
    "        hate_sample = self.sample._sample_norm(norm_mean, norm_log_sigma)\n",
    "        target_sample = self.sample._sample_gumbel_softmax(disc_log_alpha)\n",
    "        \n",
    "        # hate_sample = F.dropout(hate_sample, p=self.dropout_rate, training=self.training)\n",
    "        # target_sample = self.get_vad_loss(target_sample,vad_labels)\n",
    "        hate_logits = self.hate_classifier(hate_sample)\n",
    "\n",
    "        # target_labels = torch.zeros_like(vad_labels).float().cuda()\n",
    "        # target_labels[target_logits] = 1\n",
    "        \n",
    "        # print(\"Reconstruction loss\",reconstruct_loss,\"KL Continous Loss\",self.kl_beta_c * torch.abs(continuous_kl_loss - self.cmi),\"KL Discrete loss\",self.kl_beta_d * torch.abs(disc_kl_loss - self.dmi),end=\",\")\n",
    "        return elbo_loss_l[0], hate_logits, target_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hate_model,train_data, train_labels, train_target, val_data, val_labels, val_target, encoder_tokenizer, decoder_tokenizer, params1):\n",
    "    accumulation_steps = 16\n",
    "    train = EncodedDataset(input_sents=train_data, \n",
    "                    input_labels=train_labels, \n",
    "                    target_labels=train_target,  \n",
    "                    encoder_tokenizer=encoder_tokenizer,\n",
    "                    decoder_tokenizer=decoder_tokenizer, \n",
    "                    max_sequence_length=params1.max_sequence_length)\n",
    "    \n",
    "    val =  EncodedDataset(input_sents=val_data, \n",
    "                    input_labels=val_labels, \n",
    "                    target_labels=val_target, \n",
    "                    encoder_tokenizer=encoder_tokenizer,\n",
    "                    decoder_tokenizer=decoder_tokenizer, \n",
    "                    max_sequence_length=params1.max_sequence_length)\n",
    "    \n",
    "    \n",
    "    sampler = WeightedRandomSampler(params1.h_weights, len(params1.h_weights))\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=params1.train_batch_size,drop_last=True,sampler=sampler)\n",
    "    val_dataloader = DataLoader(val, batch_size=params1.val_batch_size,drop_last=True)\n",
    "\n",
    "    vae_optimizer = torch.optim.AdamW(hate_model.parameters(),lr=params1.content_lr,weight_decay=params1.weight_decay)\n",
    "    # hate_optimizer = torch.optim.AdamW(hate_model.hate_classifier.parameters(),lr=params1.content_lr,weight_decay=params1.weight_decay)\n",
    "    # target_optimizer = torch.optim.AdamW(hate_model.target_classifier.parameters(),lr=params1.content_lr,weight_decay=params1.weight_decay)\n",
    "    hate_loss = nn.CrossEntropyLoss(params1.hate_class_weights)\n",
    "    target_loss = nn.CrossEntropyLoss(params1.target_class_weights)\n",
    "\n",
    "    s_total_steps = float(10 * len(train)) / params1.train_batch_size\n",
    "    v_scheduler = get_linear_schedule_with_warmup(vae_optimizer, int(s_total_steps * params1.warmup_ratio),math.ceil(s_total_steps))\n",
    " \n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.9,patience=2)\n",
    "\n",
    "    # hate_opt = torch.optim.Adam(hate_model.parameters(), lr=params1.content_lr, weight_decay = 1e-2)\n",
    "    save_dir = \"D:/Hate Speech/Models/Disentanglement/WeakSupervision/\"\n",
    "    \n",
    "    best_validation_accuracy = 1e-5\n",
    "    print(\"Training started!\")\n",
    "    \n",
    "    e=0\n",
    "    for epoch in range(params1.num_epochs):\n",
    "        total_vae_loss = 0\n",
    "        total_adversary_loss = 0\n",
    "        total_cls_loss = 0\n",
    "        total_acc_train = 0\n",
    "        total_acc_target = 0\n",
    "        predictions = []\n",
    "        y_true = []\n",
    "        loss_list = []\n",
    "        predicts = []\n",
    "        ground_truth = []\n",
    "        c=0\n",
    "        cnt=0\n",
    "        hate_model.train()\n",
    "        for train_input, train_mask, train_target, train_label,train_dec_input, train_dec_mask in train_dataloader:\n",
    "            hate_model.zero_grad()\n",
    "            c+=1\n",
    "            cnt+=1\n",
    "            train_input = train_input.to(device)\n",
    "            train_mask = train_mask.to(device)\n",
    "            train_target = train_target.to(device)\n",
    "            train_label = train_label.to(device)\n",
    "            train_dec_input = train_dec_input.to(device)\n",
    "            train_dec_mask = train_dec_mask.to(device)\n",
    "            elbo_loss_l, hate_logits, target_logits = hate_model(train_input, train_mask, train_dec_input, train_dec_mask, train_input,train_target, train_label)\n",
    "            h_loss = hate_loss(hate_logits, train_label)\n",
    "            vad_loss = target_loss(target_logits,torch.argmax(train_target,dim=1))\n",
    "            # vad_loss = target_loss(target_logits, train_target)\n",
    "            # # Regularization losses\n",
    "            # l2_strength = 1e-4\n",
    "            # l2_loss = torch.tensor(0., requires_grad=True)\n",
    "            # for name, param in hate_model.named_parameters():\n",
    "            #     if 'weight' in name:\n",
    "            #         l2_loss = l2_loss + torch.norm(param, p=2)\n",
    "            # loss = h_loss + params1.alpha*elbo_loss_l + l2_strength * l2_loss + params1.target_loss_coeff*vad_loss\n",
    "            loss = params1.hate_coeff*h_loss + params1.alpha*elbo_loss_l #+ params1.target_loss_coeff*vad_loss\n",
    "            # loss = vad_loss\n",
    "            if((c+1)%accumulation_steps==0):\n",
    "                loss.backward()\n",
    "                vae_optimizer.step()\n",
    "                # hate_optimizer.step()\n",
    "                # target_optimizer.step()\n",
    "                v_scheduler.step()\n",
    "                # h_scheduler.step()\n",
    "                # t_scheduler.step()\n",
    "            \n",
    "            if(c%100==0):\n",
    "                print(\"Temperature\",hate_model.temperature.item(),\"batch\",c,\"epoch\",epoch,\"1s\",len(np.where(torch.argmax(hate_logits, dim=1).cpu().numpy()==1)[0]),\"loss\",loss.item(),\"h_loss\",h_loss.item(),\"elbo_loss\",params1.alpha*elbo_loss_l.item(),\"vad_loss\",params1.target_loss_coeff*vad_loss.item())\n",
    "            ground_truth += train_label.cpu().numpy().tolist()\n",
    "            predicts += torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()\n",
    "            loss_list.append(loss.item())\n",
    "            acc = round(accuracy_score(torch.argmax(train_label, dim=1).cpu().numpy().tolist(), torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "            target_acc = round(accuracy_score(torch.argmax(train_target, dim=1).cpu().numpy().tolist(), torch.argmax(target_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "            total_vae_loss += elbo_loss_l.item()\n",
    "            total_adversary_loss += vad_loss.item()\n",
    "            total_cls_loss += h_loss.item()\n",
    "            total_acc_train += acc\n",
    "            total_acc_target += target_acc\n",
    "            # print(\"PREDS: \",torch.argmax(target_logits, dim=1).cpu().numpy().tolist())\n",
    "            # print(\"TARGETS: \",torch.argmax(train_target, dim=1).cpu().numpy().tolist())\n",
    "            if(c%100==0):\n",
    "                print(\"Train Accuracy\",acc,\" Train Target Accuracy\",target_acc)\n",
    "            # if(c==500):\n",
    "            #     break\n",
    "        y_true = []\n",
    "        predictions = []\n",
    "        hate_model.eval()\n",
    "        total_loss_val = 0\n",
    "        total_acc_val = 0\n",
    "        total_acc_target_val = 0\n",
    "        e_cnt=0\n",
    "        print(\"VALIDATION\")\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_mask, val_target, val_label, val_dec_input, val_dec_mask in val_dataloader:\n",
    "                e_cnt+=1\n",
    "                val_input = val_input.to(device)\n",
    "                val_mask = val_mask.to(device)\n",
    "                val_target = val_target.to(device)\n",
    "                val_label = val_label.to(device)\n",
    "                val_dec_input = val_dec_input.to(device)\n",
    "                val_dec_mask = val_dec_mask.to(device)\n",
    "                elbo_loss_l, hate_logits, target_logits = hate_model(val_input, val_mask, val_dec_input, val_dec_mask, val_input,val_target, val_label)\n",
    "                total_acc_val += round(accuracy_score(torch.argmax(val_label, dim=1).cpu().numpy().tolist(), torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "                total_acc_target_val += round(accuracy_score(torch.argmax(val_target, dim=1).cpu().numpy().tolist(), torch.argmax(target_logits,dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "                y_true += torch.argmax(val_label, dim=1).cpu().numpy().tolist()\n",
    "                predictions += torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()\n",
    "            print(classification_report(y_true, predictions))\n",
    "\n",
    "        metrics = {\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train ELBO Loss\" : total_vae_loss/cnt,\n",
    "            \"Train Target Loss\" : total_adversary_loss/cnt,\n",
    "            \"Train Hate Loss\" : total_cls_loss/cnt,\n",
    "            \"Train Hate Accuracy\" : total_acc_train/cnt,\n",
    "            \"Train Target Accuracy\" : total_acc_target/cnt,\n",
    "            \"Validation Loss\" : total_loss_val/e_cnt,\n",
    "            \"Validation Hate Accuracy\" : total_acc_val/e_cnt,\n",
    "            \"Validation Target Accuracy\" : total_acc_target_val/e_cnt}\n",
    "\n",
    "        print(metrics)\n",
    "        wandb.log(metrics)\n",
    "        \n",
    "        val_metrics = {\"F1-Score\": classification_report(y_true, predictions,output_dict=True)['macro avg']['f1-score']}\n",
    "        wandb.log({**metrics, **val_metrics})\n",
    "\n",
    "        if(best_validation_accuracy <= round(classification_report(y_true, predictions,output_dict=True)['macro avg']['f1-score'],3)):\n",
    "            best_validation_accuracy = round(classification_report(y_true, predictions,output_dict=True)['macro avg']['f1-score'],3)\n",
    "            best_report = classification_report(y_true, predictions)\n",
    "            e=0\n",
    "            print(\"E from if = \",e)\n",
    "            # fname = \"best-model_\" + params1.dataset_name+\"_\"+str(epoch+1)+\"_VAE_with_rob_base_NoFT_balancedData.pt\"\n",
    "            fname = \"best-model_\" + params1.dataset_name+\"_\"+str(epoch+1)+params1.fname\n",
    "            torch.save(hate_model.state_dict(), os.path.join(save_dir, fname))\n",
    "            print(\"Saved at \",os.path.join(save_dir, fname))\n",
    "        else:\n",
    "            print(\"E = \",e)\n",
    "            e+=1\n",
    "        if(e==1):\n",
    "            print(best_report)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "files = ['D:/Hate Speech/Preprocessed Datasets/GAB/gab_HX_Numeric_train.csv',\n",
    "           'D:/Hate Speech/Preprocessed Datasets/Reddit/reddit_TRY_Numeric_train.csv',\n",
    "           'D:/Hate Speech/Preprocessed Datasets/Twitter/twitter_HX_Numeric_train.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_train.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_Numeric_train.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_Numeric_train.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_train.csv',\n",
    "            'D:/Hate Speech/Preprocessed Datasets/YouTube/youtube_IC_Numeric_train.csv']\n",
    "\n",
    "test_files = ['D:/Hate Speech/Preprocessed Datasets/GAB/gab_HX_Numeric_test.csv',\n",
    "          'D:/Hate Speech/Preprocessed Datasets/Reddit/reddit_TRY_Numeric_test.csv',\n",
    "           'D:/Hate Speech/Preprocessed Datasets/Twitter/twitter_HX_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_Numeric_test.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_Numeric_test.csv',\n",
    "        #     './Preprocessed Datasets/Youtube/youtube_TRY_Numeric_test.csv',\n",
    "            'D:/Hate Speech/Preprocessed Datasets/YouTube/youtube_IC_Numeric_test.csv']\n",
    "\n",
    "enc_files = ['D:/Hate Speech/Finetuned/Roberta/gab_HX/pytorch_model.bin',\n",
    "            # 'D:/Hate Speech/Finetuned/Roberta/reddit_TRY/pytorch_model.bin',\n",
    "            # 'D:/Hate Speech/Finetuned/Roberta/twitter_HX/pytorch_model.bin',\n",
    "            # # './FineTuned/Roberta/twitter_TRY/pytorch_model.bin',\n",
    "            # # './FineTuned/Roberta/twitter/pytorch_model.bin',\n",
    "            # # './FineTuned/Roberta/youtube/pytorch_model.bin',\n",
    "            # # './FineTuned/Roberta/youtube_TRY/pytorch_model.bin',\n",
    "            'D:/Hate Speech/Finetuned/Roberta/youtube_IC/pytorch_model.bin']\n",
    "\n",
    "# enc_files = ['./FineTuned/Roberta/gab/pytorch_model.bin',\n",
    "#              './FineTuned/RobertaHate/reddit_TRY/pytorch_model.bin',\n",
    "#              './FineTuned/RobertaHate/twitter/pytorch_model.bin',\n",
    "#              './FineTuned/RobertaHate/youtube/pytorch_model.bin']\n",
    "\n",
    "dec_files = ['D:/Hate Speech/Finetuned/BART/pytorch_model.bin',\n",
    "            #  'D:/Hate Speech/Finetuned/BART_reddit_TRY/checkpoint-7000/pytorch_model.bin',\n",
    "            #  'D:/Hate Speech/Finetuned/BART_twitter/pytorch_model.bin',\n",
    "             'D:/Hate Speech/Finetuned/BART_youtube/pytorch_model.bin']\n",
    "\n",
    "\n",
    "dataset_names = [\"gab_HX\",\"youtube_IC\"]#\"reddit_TRY\",\"twitter_HX\",\"youtube_IC\"]#\"twitter_TRY\",\"twitter\",\"youtube\",\"youtube_TRY\",\n",
    "hidden_size = 768\n",
    "classifier_dropout = 0.2\n",
    "learning_rate = 5e-3\n",
    "print(files)\n",
    "print(len(files))\n",
    "print(device)\n",
    "filenames = set()\n",
    "latent_variables = ['hate','target']#['0','1','2','3','4','5','6','7']\n",
    "\n",
    "for f in range(0,len(files)):\n",
    "        torch.cuda.empty_cache()\n",
    "        train_frame = pd.read_csv(files[f])\n",
    "        train_frame['target'] = train_frame['target'].astype('int32')\n",
    "        tar = np.zeros(8)\n",
    "        a1 = np.unique(train_frame['target'])\n",
    "        class_weights = compute_class_weight('balanced', classes=a1, y=train_frame['target'])\n",
    "        for i in range(len(a1)):\n",
    "            try:\n",
    "                tar[int(a1[i])] = class_weights[a1[i]]\n",
    "            except:\n",
    "                tar[int(a1[i])] = 0\n",
    "        print(tar)\n",
    "        class_weights2 = torch.FloatTensor(tar).to(device)\n",
    "        print(class_weights2)\n",
    "        if(dataset_names[f] not in filenames):\n",
    "            filenames.add(dataset_names[f])\n",
    "            # train_frame = pd.read_csv(files[f])\n",
    "            if(f+1<len(files)):\n",
    "                print(\"TEST FILE: \",test_files[f+1])\n",
    "                test_frame = pd.read_csv(test_files[f+1])\n",
    "            else:\n",
    "                 test_frame = pd.read_csv(test_files[0])\n",
    "            class_weights1 = compute_class_weight('balanced', classes=np.unique(train_frame['label']), y=train_frame['label'])\n",
    "            class_weights1 = torch.FloatTensor(class_weights1)\n",
    "            x = train_frame['label'].value_counts().values\n",
    "            print(x)\n",
    "            class_weights3 = torch.FloatTensor([x[0]/sum(x),x[1]/sum(x)])\n",
    "            # class_weights3 = torch.FloatTensor([0.6,0.4])\n",
    "            h_weights = class_weights3[train_frame['label']]\n",
    "            print(h_weights)\n",
    "            class_weights1 = class_weights1.to(device)\n",
    "            wandb.init(\n",
    "                # Set the project where this run will be logged\n",
    "                project=\"Disentangling Hate Speech and Target\", \n",
    "                # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "                name=f\"experiment_ROB_BART_{dataset_names[-1]}_{classifier_dropout}_{learning_rate}\",\n",
    "                # Track hyperparameters and run metadata\n",
    "                config={\n",
    "                        \"max_sequence_length\": 512, \n",
    "                        \"train_batch_size\" : 8, \n",
    "                        \"val_batch_size\" : 8,\n",
    "                        \"hidden_dim\" : 512, \n",
    "                        \"hate_dim\" : 384,\n",
    "                        \"num_epochs\" : 100, \n",
    "                        \"device\" : device,\n",
    "                        \"dataset_name\" : dataset_names[f],\n",
    "                        \"h_weights\" : h_weights,\n",
    "                        \"hate_class_weights\" : class_weights1,\n",
    "                        \"target_class_weights\" : class_weights2,\n",
    "                        \"hidden_size\" : 768,\n",
    "                        \"num_labels\": 2,\n",
    "                        \"num_targets\": 8,\n",
    "                        \"classifier_dropout\" : classifier_dropout,\n",
    "                        \"content_lr\": 2e-5,\n",
    "                        \"decoder_type\" : \"BART\",\n",
    "                        \"kl_weight\" : 0.05,\n",
    "                        \"mi_loss_weight\" : 0.001,\n",
    "                        \"mi_loss\" : False,\n",
    "                        \"alpha\" : 1,\n",
    "                        \"beta_c\" : 0.05,\n",
    "                        \"beta_d\" : 0.05,\n",
    "                        \"warmup_ratio\" : 0.2,\n",
    "                        \"weight_decay\" : 0.001,\n",
    "                        \"target_loss_coeff\" : 1,\n",
    "                        \"hate_coeff\" : 1,\n",
    "                        \"fname\" : \"_VAE_with_rob_base_NoFT_balancedData_US.pt\",\n",
    "                        })\n",
    "            params1 = { \n",
    "                        \"max_sequence_length\": 512, \n",
    "                        \"train_batch_size\" : 8, \n",
    "                        \"val_batch_size\" : 8,\n",
    "                        \"hidden_dim\" : 512, \n",
    "                        \"hate_dim\" : 384,\n",
    "                        \"num_epochs\" : 100, \n",
    "                        \"device\" : device,\n",
    "                        \"dataset_name\" : dataset_names[f],\n",
    "                        \"h_weights\" : h_weights,\n",
    "                        \"hate_class_weights\" : class_weights1,\n",
    "                        \"target_class_weights\" : class_weights2,\n",
    "                        \"hidden_size\" : 768,\n",
    "                        \"num_labels\": 2,\n",
    "                        \"num_targets\": 8,\n",
    "                        \"classifier_dropout\" : classifier_dropout,\n",
    "                        \"content_lr\": 2e-5,\n",
    "                        \"decoder_type\" : \"BART\",\n",
    "                        \"kl_weight\" : 0.05,\n",
    "                        \"mi_loss_weight\" : 0.001,\n",
    "                        \"mi_loss\" : False,\n",
    "                        \"alpha\" : 1,\n",
    "                        \"beta_c\" : 0.05,\n",
    "                        \"beta_d\" : 0.05,\n",
    "                        \"warmup_ratio\" : 0.2,\n",
    "                        \"weight_decay\" : 0.001,\n",
    "                        \"target_loss_coeff\" : 1,\n",
    "                        \"hate_coeff\" : 1,\n",
    "                        \"fname\" : \"_VAE_with_rob_base_NoFT_balancedData_US.pt\",\n",
    "                    }\n",
    "            \n",
    "            params1 = SimpleNamespace(**params1)\n",
    "            # enc_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "            enc_model_name = \"roberta-base\"\n",
    "            dec_model_name = \"facebook/bart-base\"\n",
    "            # enc_chk = \"./FineTuned/Roberta/pytorch_model.bin\"\n",
    "            print(enc_files[f])\n",
    "            enc_chk = enc_files[f]#\"./FineTuned/RobertaHate/pytorch_model.bin\"\n",
    "            dec_chk = dec_files[f]#\"./FineTuned/BART/pytorch_model.bin\"\n",
    "            hate_model = CATCH(1.0,enc_chk,enc_model_name,dec_chk,dec_model_name, params1.num_targets,params1.num_labels,params1.beta_c, params1.beta_d,device,params1.train_batch_size,params1.decoder_type).to(device)\n",
    "            encoder_tokenizer = RobertaTokenizer.from_pretrained(enc_model_name)\n",
    "            decoder_tokenizer = BartTokenizer.from_pretrained(dec_model_name)\n",
    "            print(params1.dataset_name)\n",
    "            print(files[f])\n",
    "            print(train_frame.shape)\n",
    "            wandb.watch(hate_model)\n",
    "            # train(hate_model=hate_model,\n",
    "            # train_data=train_frame['text'].values.tolist(), \n",
    "            # train_labels=train_frame['label'].values.tolist(), \n",
    "            # train_target=train_frame['target'].values.tolist(), \n",
    "            # val_data=test_frame['text'].values.tolist(), \n",
    "            # val_labels=test_frame['label'].values.tolist(), \n",
    "            # val_target = test_frame['target'].values.tolist(),\n",
    "            # encoder_tokenizer=encoder_tokenizer,\n",
    "            # decoder_tokenizer=decoder_tokenizer,\n",
    "            # params1=params1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data, test_labels, target_labels, encoder_tokenizer, decoder_tokenizer, params1):\n",
    "    \n",
    "    test = EncodedDataset(input_sents=test_data, \n",
    "                    input_labels=test_labels, \n",
    "                    target_labels=target_labels,  \n",
    "                    encoder_tokenizer=encoder_tokenizer,\n",
    "                    decoder_tokenizer=decoder_tokenizer,\n",
    "                    max_sequence_length=params1.max_sequence_length)\n",
    "    \n",
    "\n",
    "    val_dataloader = DataLoader(test, batch_size=params1.val_batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    predictions = []\n",
    "    y_true = []\n",
    "    total_acc_val = 0\n",
    "    total_acc_target_val = 0\n",
    "    hate_model.eval()\n",
    "    e_cnt=0\n",
    "    with torch.no_grad():\n",
    "      for val_input, val_mask, val_target, val_label, val_dec_input, val_dec_mask in val_dataloader:\n",
    "        e_cnt+=1\n",
    "        val_input = val_input.to(device)\n",
    "        val_mask = val_mask.to(device)\n",
    "        val_target = val_target.to(device)\n",
    "        val_label = val_label.to(device)\n",
    "        val_dec_input = val_dec_input.to(device)\n",
    "        val_dec_mask = val_dec_mask.to(device)\n",
    "        elbo_loss_l, hate_logits, target_logits = hate_model(val_input, val_mask, val_dec_input, val_dec_mask, val_input,val_target, val_label)\n",
    "        total_acc_val += round(accuracy_score(torch.argmax(val_label, dim=1).cpu().numpy().tolist(), torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "        #total_acc_target_val += round(accuracy_score(torch.argmax(val_target, dim=1).cpu().numpy().tolist(), torch.argmax(target_logits,dim=1).cpu().numpy().tolist()) * 100, 3)\n",
    "        y_true += torch.argmax(val_label, dim=1).cpu().numpy().tolist()\n",
    "        predictions += torch.argmax(hate_logits, dim=1).cpu().numpy().tolist()\n",
    "      print(classification_report(y_true, predictions))\n",
    "      print(\"Accuracy: \", total_acc_val/e_cnt)\n",
    "      #print(\"Target Accuracy: \", total_acc_target_val/e_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = [\"D:/Hate Speech/Models/Disentanglement/WeakSupervision/best-model_youtube_IC_1_VAE_with_rob_base_NoFT_balancedData_US.pt\",\n",
    "               \"D:/Hate Speech/Models/Disentanglement/WeakSupervision/best-model_gab_HX_7_VAE_with_rob_base_NoFT_balancedData_US.pt\"]\n",
    "\n",
    "test_files =['D:/Hate Speech/Preprocessed Datasets/GAB/gab_HX_Numeric_test.csv',\n",
    "          'D:/Hate Speech/Preprocessed Datasets/Reddit/reddit_TRY_Numeric_test.csv',\n",
    "           'D:/Hate Speech/Preprocessed Datasets/Twitter/twitter_HX_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_TRY_Numeric_test.csv',\n",
    "        #    './Preprocessed Datasets/Twitter/twitter_Numeric_test.csv',\n",
    "              'D:/Hate Speech/Preprocessed Datasets/Youtube/youtube_Numeric_test.csv']\n",
    "        #     'D:/Hate Speech/Preprocessed Datasets/Youtube/youtube_TRY_Numeric_test.csv']\n",
    "        #    'D:/Hate Speech/Preprocessed Datasets/Youtube/youtube_IC_Numeric_test.csv']\n",
    "# test_files = ['./reddit_TRY_Numeric_test_subset.csv']\n",
    "\n",
    "for f in model_files:\n",
    "        print(f)\n",
    "        print(\"*\"*100)\n",
    "        for f1 in range(0,len(test_files)):\n",
    "                print(test_files[f1])\n",
    "                print(\"*\"*100)\n",
    "                hate_model.load_state_dict(torch.load(f))\n",
    "                test_frame =  pd.read_csv(test_files[f1])\n",
    "                evaluate(model=hate_model, test_data = test_frame['text'].values.tolist(), test_labels=test_frame['label'].values.tolist(),target_labels=test_frame['target'].values.tolist(), encoder_tokenizer=encoder_tokenizer,decoder_tokenizer=decoder_tokenizer,params1=params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
